{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llamaindex transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scraper_starturl': 'https://api.mycareersfuture.gov.sg/v2/search?limit=20&page=0', 'scraper_results_file': './data/scraper_results.json', 'scraper_delay': 0.5, 'scraper_query': {'sessionId': '', 'search': 'data', 'salary': 6000, 'positionLevels': ['Executive', 'Junior Executive', 'Fresh/entry level'], 'postingCompany': []}, 'user_resume_txt_file': './data/resume.txt', 'similarity_top_k': 20, 'instruction_prompt': \"Which of the given job listings best match the user's resume below? Return the company name for the top 5 best fits.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 15:18:31,332 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2023-11-10 15:18:31,729 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "import requests\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index import Document, ServiceContext, StorageContext, VectorStoreIndex\n",
    "from llama_index.embeddings import LangchainEmbedding  # BertEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "from src.mycareersfuture import MyCareersFutureListings\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def read_yaml_config(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "file_path = \"conf/base/config.yml\"\n",
    "config = read_yaml_config(file_path)\n",
    "print(config)\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"LLAMA_INDEX_CACHE_DIR\"] = \"cache\"\n",
    "\n",
    "\n",
    "mcf_listings = MyCareersFutureListings(sleep_delay=config[\"scraper_delay\"])\n",
    "\n",
    "listings = mcf_listings.load_json(json_load_file=config[\"scraper_results_file\"])\n",
    "\n",
    "\n",
    "### REDUCE DATASET TO RELEVANT FIELDS ###\n",
    "reduced = []\n",
    "for listing in listings:\n",
    "    reduced.append(\n",
    "        {\n",
    "            \"url\": listing[\"metadata\"][\"jobDetailsUrl\"],\n",
    "            \"job_title\": listing[\"title\"],\n",
    "            \"job_desc\": listing[\"job_desc\"],\n",
    "            \"company\": listing[\"postedCompany\"][\"name\"],\n",
    "            \"salary_min\": listing[\"salary\"][\"minimum\"],\n",
    "            \"salary_max\": listing[\"salary\"][\"maximum\"],\n",
    "            \"skills\": \", \".join([skill[\"skill\"] for skill in listing[\"skills\"]]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "### CREATE DOCUMENTS FROM ALL THE RETURNED LISTINGS ###\n",
    "documents = [\n",
    "    Document(\n",
    "        text=listing[\"job_desc\"],\n",
    "        metadata={\n",
    "            \"url\": listing[\"url\"],\n",
    "            \"job_title\": listing[\"job_title\"],\n",
    "            \"company\": listing[\"company\"],\n",
    "            \"salary_min\": listing[\"salary_min\"],\n",
    "            \"salary_max\": listing[\"salary_max\"],\n",
    "            \"skills\": listing[\"skills\"],\n",
    "        },\n",
    "        excluded_llm_metadata_keys=[\"url\", \"salary_min\", \"salary_max\"],\n",
    "        excluded_embed_metadata_keys=[\"url\", \"salary_min\", \"salary_max\"],\n",
    "        metadata_separator=\"::\",\n",
    "        metadata_template=\"{key}->{value}\",\n",
    "        text_template=\"Job Listing Metadata: {metadata_str}\\n-----\\nJob Listing: {content}\\n-----\\n\",\n",
    "    )\n",
    "    for listing in reduced\n",
    "]\n",
    "\n",
    "### CREATE VECTOR STORE ###\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "service_context_embedding = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context_embedding,\n",
    ")\n",
    "\n",
    "### SET UP SERVICE CONTEXT ###\n",
    "service_context_llm = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "### CREATE RETRIEVER ###\n",
    "retriever = index.as_retriever(similarity_top_k=config[\"similarity_top_k\"])\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    "    service_context=service_context_llm,\n",
    "    use_async=False,\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "### CREATE QUERY ENGINE ###\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever, response_synthesizer=response_synthesizer\n",
    ")\n",
    "\n",
    "### LOAD USER'S RESUME ###\n",
    "with open(config[\"user_resume_txt_file\"], \"r\", encoding=\"utf8\") as file:\n",
    "    user_resume = file.read()\n",
    "\n",
    "### PROMPT TEMPLATE ###\n",
    "user_input = (\n",
    "    f\"INSTRUCTION:\\n{config['instruction_prompt']}\\n\\n RESUME:\\n{user_resume}\\n\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.89it/s]\n",
      "2023-11-10 15:23:32,147 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 15:23:34,697 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 15:23:43,945 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 15:23:53,426 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 15:24:04,533 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 15:24:15,095 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 15:24:26,103 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 15:24:37,419 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 15:24:48,297 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 15:24:58,275 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the user's job experience, the top 3 job listings that are the best match are:\n",
      "\n",
      "1. Company: AI Singapore\n",
      "   - Job Title: Data Scientist\n",
      "   - Duration: Feb 2023 - Nov 2023\n",
      "   - Key Responsibilities:\n",
      "     - Collaborated closely with a multidisciplinary team and effectively communicated findings and insights to both technical and non-technical stakeholders.\n",
      "     - Conducted exploratory data analysis and feature engineering on audio datasets.\n",
      "     - Authored a data engineering and orchestration pipeline for ETL processes on audio datasets.\n",
      "     - Conducted literature review of current state-of-the-art models and performed model selection and evaluation.\n",
      "     - Developed a scalable model-serving application using FastAPI, Celery, RabbitMQ, Redis, Docker, and Kubernetes.\n",
      "     - Mentored junior apprentices during their deep-skilling phase.\n",
      "\n",
      "2. Company: Singapore Association of Motion Pictures Professionals\n",
      "   - Job Title: Data Analyst/Research Coordinator\n",
      "   - Duration: 2019 - 2023\n",
      "   - Key Responsibilities:\n",
      "     - Coordinated focus group studies and townhall meetups for media industry practitioners.\n",
      "     - Conducted exploratory data analysis, data cleaning, aggregation, visualizations, and report creation.\n",
      "     - Assisted executive committee in mapping out roadmap and key objectives for the association.\n",
      "     - Consulted on database selection, data gathering, and analytics-related decisions for the association's jobs portal platform.\n",
      "\n",
      "3. Company: Singapore Association of Motion Pictures Professionals\n",
      "   - Job Title: Web Designer/Webmaster/Copywriter\n",
      "   - Duration: 2019 - 2020\n",
      "   - Key Responsibilities:\n",
      "     - Designed and maintained association website using Wordpress and jQuery.\n",
      "     - Planned and copywrote content for microsite to educate media practitioners on various issues.\n",
      "     - Setup and maintained domain, webhost accounts, and linked APIs for the website.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### RUN QUERY ###\n",
    "result = query_engine.query(user_input)\n",
    "print(f\"Answer: {str(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index import Document, ServiceContext, StorageContext, VectorStoreIndex\n",
    "from llama_index.embeddings import LangchainEmbedding  # BertEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "from src.mycareersfuture import MyCareersFutureListings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 14:34:35,398 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "c:\\ProgramData\\Anaconda3\\envs\\llamaindex\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-10 14:34:41,142 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2023-11-10 14:34:42,770 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.38it/s]\n",
      "2023-11-10 14:34:50,104 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 14:34:53,309 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 14:34:55,214 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2023-11-10 14:34:57,270 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the skills and experience listed in the resume, the top 3 job listings that best match the user's skills are:\n",
      "\n",
      "1. FINERGIC SOLUTIONS PTE. LTD. - Data Engineer\n",
      "2. ABC Company - Data Engineer\n",
      "3. XYZ Corporation - Data Engineer\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"LLAMA_INDEX_CACHE_DIR\"] = \"cache\"\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "JSON_LOAD_FILE = \"./jobslist.json\"\n",
    "SLEEP_DELAY = 0.5\n",
    "\n",
    "mcf_listings = MyCareersFutureListings(sleep_delay=SLEEP_DELAY)\n",
    "listings = mcf_listings.load_json(json_load_file=JSON_LOAD_FILE)\n",
    "\n",
    "reduced = []\n",
    "for listing in listings:\n",
    "    reduced.append(\n",
    "        {\n",
    "            \"url\": listing[\"metadata\"][\"jobDetailsUrl\"],\n",
    "            \"job_title\": listing[\"title\"],\n",
    "            \"job_desc\": listing[\"job_desc\"],\n",
    "            \"company\": listing[\"postedCompany\"][\"name\"],\n",
    "            \"salary_min\": listing[\"salary\"][\"minimum\"],\n",
    "            \"salary_max\": listing[\"salary\"][\"maximum\"],\n",
    "            \"skills\": \", \".join([skill[\"skill\"] for skill in listing[\"skills\"]]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        text=listing[\"job_desc\"],\n",
    "        metadata={\n",
    "            \"url\": listing[\"url\"],\n",
    "            \"job_title\": listing[\"job_title\"],\n",
    "            \"company\": listing[\"company\"],\n",
    "            \"salary_min\": listing[\"salary_min\"],\n",
    "            \"salary_max\": listing[\"salary_max\"],\n",
    "            \"skills\": listing[\"skills\"],\n",
    "        },\n",
    "        excluded_llm_metadata_keys=[\"url\", \"salary_min\", \"salary_max\"],\n",
    "        excluded_embed_metadata_keys=[\"url\", \"salary_min\", \"salary_max\"],\n",
    "        metadata_separator=\"::\",\n",
    "        metadata_template=\"{key}->{value}\",\n",
    "        text_template=\"Metadata: {metadata_str}\\n-----\\nJob Listing: {content}\",\n",
    "    )\n",
    "    for listing in reduced\n",
    "]\n",
    "\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "service_context_embedding = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "index=VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context_embedding,\n",
    ")\n",
    "\n",
    "service_context_llm = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.1,\n",
    "    ),\n",
    "    # system_prompt=\"You are an AI assistant assisting job seekers find the best matches based on their profile.\"\n",
    ")\n",
    "# retriever = VectorIndexRetriever(\n",
    "#     index=index,\n",
    "#     similarity_top_k=10,\n",
    "# )\n",
    "retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    "    service_context=service_context_llm,\n",
    "    use_async=False,\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever, response_synthesizer=response_synthesizer\n",
    ")\n",
    "\n",
    "with open('./data/resume.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "user_input = f\"Which of the retrieved job listings best match the user's resume below? Return the company name for the top 3 best fits.\\n\\n RESUME:\\n{content}\\n\"\n",
    "result = query_engine.query(user_input)\n",
    "print(f\"Answer: {str(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='dbe92a83-2528-4d48-bce5-ead57fafb9e4', embedding=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd', 'job_title': 'Data Analyst/Data Engineer', 'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'salary_min': 5000, 'salary_max': 7000, 'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}, excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'], excluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='37210b26-5b33-469e-aadd-0a45aa4e6998', node_type=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd', 'job_title': 'Data Analyst/Data Engineer', 'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'salary_min': 5000, 'salary_max': 7000, 'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}, hash='7ca6f54562f6f797114f1f9cb150ae3149840908f716e571e9e9abdcf321890a')}, hash='84b8fdb30dbacd95ae1779e1818bcb696ac4dd1cda10aa50eecbe46cce820acc', text='Metadata: \\n-----\\nContent: Metadata: \\n-----\\nContent: Job Description:\\n\\n  Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\\n\\nRequired and desired skills/qualifications:\\n\\n  Desired candidate should be having around 3-5 years of experience.\\n  Have strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\\n  Experience in monitoring, Tuning tasks on Cloudera distribution.\\n  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\\n  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\\n  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\\n  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\\n  Good experience working with different Hadoop file formats like Sequence File, ORC, AVRO and Parquet.\\n  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\\n  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\\n  In-depth understanding of Data Structure and Algorithms.\\n  Implemented in setting up standards and processes for Hadoop based application design and implementation.\\n  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\\n  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\\n  Good experience in CI/CD pipeline and working in Agile environment.\\n  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\\n  Developed analytical components using SparkSql and Spark Stream.\\n  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\\n  Good knowledge streaming data using Kafka from multiple sources into HDFS.\\n  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\\n  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications\\n\\n Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development', start_char_idx=0, end_char_idx=3102, text_template='Metadata: {metadata_str}\\n-----\\nContent: {content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3422720888707207),\n",
       " NodeWithScore(node=TextNode(id_='5cd52d83-7162-4caa-9cfe-f600402e8a54', embedding=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd', 'job_title': 'Data Analyst/Data Engineer', 'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'salary_min': 5000, 'salary_max': 7000, 'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}, excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'], excluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fc2b4bab-00c6-4402-b266-ca4ae42863fa', node_type=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd', 'job_title': 'Data Analyst/Data Engineer', 'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'salary_min': 5000, 'salary_max': 7000, 'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}, hash='7ca6f54562f6f797114f1f9cb150ae3149840908f716e571e9e9abdcf321890a')}, hash='84b8fdb30dbacd95ae1779e1818bcb696ac4dd1cda10aa50eecbe46cce820acc', text='Metadata: \\n-----\\nContent: Metadata: \\n-----\\nContent: Job Description:\\n\\n  Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\\n\\nRequired and desired skills/qualifications:\\n\\n  Desired candidate should be having around 3-5 years of experience.\\n  Have strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\\n  Experience in monitoring, Tuning tasks on Cloudera distribution.\\n  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\\n  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\\n  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\\n  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\\n  Good experience working with different Hadoop file formats like Sequence File, ORC, AVRO and Parquet.\\n  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\\n  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\\n  In-depth understanding of Data Structure and Algorithms.\\n  Implemented in setting up standards and processes for Hadoop based application design and implementation.\\n  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\\n  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\\n  Good experience in CI/CD pipeline and working in Agile environment.\\n  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\\n  Developed analytical components using SparkSql and Spark Stream.\\n  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\\n  Good knowledge streaming data using Kafka from multiple sources into HDFS.\\n  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\\n  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications\\n\\n Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development', start_char_idx=0, end_char_idx=3102, text_template='Metadata: {metadata_str}\\n-----\\nContent: {content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3422720888707207),\n",
       " NodeWithScore(node=TextNode(id_='bb88e4d4-0a99-4bf4-b83c-8e2be82e7373', embedding=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd', 'job_title': 'Data Analyst/Data Engineer', 'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'salary_min': 5000, 'salary_max': 7000, 'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}, excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'], excluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2dec2bc-06d2-49bf-b784-1d4758cf9570', node_type=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd', 'job_title': 'Data Analyst/Data Engineer', 'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'salary_min': 5000, 'salary_max': 7000, 'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}, hash='7ca6f54562f6f797114f1f9cb150ae3149840908f716e571e9e9abdcf321890a')}, hash='84b8fdb30dbacd95ae1779e1818bcb696ac4dd1cda10aa50eecbe46cce820acc', text='Metadata: \\n-----\\nContent: Metadata: \\n-----\\nContent: Job Description:\\n\\n  Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\\n\\nRequired and desired skills/qualifications:\\n\\n  Desired candidate should be having around 3-5 years of experience.\\n  Have strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\\n  Experience in monitoring, Tuning tasks on Cloudera distribution.\\n  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\\n  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\\n  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\\n  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\\n  Good experience working with different Hadoop file formats like Sequence File, ORC, AVRO and Parquet.\\n  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\\n  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\\n  In-depth understanding of Data Structure and Algorithms.\\n  Implemented in setting up standards and processes for Hadoop based application design and implementation.\\n  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\\n  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\\n  Good experience in CI/CD pipeline and working in Agile environment.\\n  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\\n  Developed analytical components using SparkSql and Spark Stream.\\n  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\\n  Good knowledge streaming data using Kafka from multiple sources into HDFS.\\n  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\\n  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications\\n\\n Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development', start_char_idx=0, end_char_idx=3102, text_template='Metadata: {metadata_str}\\n-----\\nContent: {content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3422720888707207),\n",
       " NodeWithScore(node=TextNode(id_='6605a81f-a4dd-432f-ab08-e4db40b8ad9c', embedding=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd', 'job_title': 'Data Analyst/Data Engineer', 'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'salary_min': 5000, 'salary_max': 7000, 'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}, excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'], excluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='70a74cb8-314e-4753-a8a2-01a5dd1053ef', node_type=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd', 'job_title': 'Data Analyst/Data Engineer', 'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'salary_min': 5000, 'salary_max': 7000, 'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}, hash='7ca6f54562f6f797114f1f9cb150ae3149840908f716e571e9e9abdcf321890a')}, hash='84b8fdb30dbacd95ae1779e1818bcb696ac4dd1cda10aa50eecbe46cce820acc', text='Metadata: \\n-----\\nContent: Metadata: \\n-----\\nContent: Job Description:\\n\\n  Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\\n\\nRequired and desired skills/qualifications:\\n\\n  Desired candidate should be having around 3-5 years of experience.\\n  Have strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\\n  Experience in monitoring, Tuning tasks on Cloudera distribution.\\n  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\\n  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\\n  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\\n  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\\n  Good experience working with different Hadoop file formats like Sequence File, ORC, AVRO and Parquet.\\n  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\\n  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\\n  In-depth understanding of Data Structure and Algorithms.\\n  Implemented in setting up standards and processes for Hadoop based application design and implementation.\\n  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\\n  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\\n  Good experience in CI/CD pipeline and working in Agile environment.\\n  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\\n  Developed analytical components using SparkSql and Spark Stream.\\n  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\\n  Good knowledge streaming data using Kafka from multiple sources into HDFS.\\n  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\\n  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications\\n\\n Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development', start_char_idx=0, end_char_idx=3102, text_template='Metadata: {metadata_str}\\n-----\\nContent: {content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3422720888707207),\n",
       " NodeWithScore(node=TextNode(id_='4c4e0d67-03a3-4e3c-bd80-eb3f926d0a55', embedding=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd', 'job_title': 'Data Analyst/Data Engineer', 'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'salary_min': 5000, 'salary_max': 7000, 'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}, excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'], excluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5021ec93-7857-4524-b3a9-098a3fb077e6', node_type=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd', 'job_title': 'Data Analyst/Data Engineer', 'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'salary_min': 5000, 'salary_max': 7000, 'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}, hash='7ca6f54562f6f797114f1f9cb150ae3149840908f716e571e9e9abdcf321890a')}, hash='84b8fdb30dbacd95ae1779e1818bcb696ac4dd1cda10aa50eecbe46cce820acc', text='Metadata: \\n-----\\nContent: Metadata: \\n-----\\nContent: Job Description:\\n\\n  Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\\n\\nRequired and desired skills/qualifications:\\n\\n  Desired candidate should be having around 3-5 years of experience.\\n  Have strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\\n  Experience in monitoring, Tuning tasks on Cloudera distribution.\\n  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\\n  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\\n  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\\n  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\\n  Good experience working with different Hadoop file formats like Sequence File, ORC, AVRO and Parquet.\\n  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\\n  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\\n  In-depth understanding of Data Structure and Algorithms.\\n  Implemented in setting up standards and processes for Hadoop based application design and implementation.\\n  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\\n  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\\n  Good experience in CI/CD pipeline and working in Agile environment.\\n  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\\n  Developed analytical components using SparkSql and Spark Stream.\\n  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\\n  Good knowledge streaming data using Kafka from multiple sources into HDFS.\\n  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\\n  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications\\n\\n Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development', start_char_idx=0, end_char_idx=3102, text_template='Metadata: {metadata_str}\\n-----\\nContent: {content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3422720888707207),\n",
       " NodeWithScore(node=TextNode(id_='b0a47c28-23f9-4e36-ad62-37c9404482d2', embedding=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-finergic-solutions-873f05c6b856dd6cd4aad3d66081217c', 'job_title': 'Data Engineer', 'company': 'FINERGIC SOLUTIONS PTE. LTD.', 'salary_min': 4000, 'salary_max': 6000, 'skills': 'Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases'}, excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'], excluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='863020c7-b49c-42e8-b27a-0f966dd04deb', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-finergic-solutions-873f05c6b856dd6cd4aad3d66081217c', 'job_title': 'Data Engineer', 'company': 'FINERGIC SOLUTIONS PTE. LTD.', 'salary_min': 4000, 'salary_max': 6000, 'skills': 'Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases'}, hash='5e2172e388b8f8723435c7df6206bd6b97054c9f01eaf996787ce060a9285cc9')}, hash='5a7e002bdc68ad8939f7283f6e28244626a1bcc3bfbc865728a651664b783e16', text='Who we are\\nWe are a boutique management consulting firm offering Business Consulting, Advisory, and Technology services to our banking clients in the wealth space. We partner with our clients on large transformation projects and act as their delivery partners.\\nWe are building a competency center with a high-potential team with niche skill sets. Our focus is to bring together, a diverse talent pool that will create perfect synergy in delivering the best possible solutions for the projects we undertake. We also aspire to provide an inclusive workplace where fresh talent can grow and achieve their full potential.\\nJob Description\\nThe candidate must have 5+ years of experience in a Data Engineer role with a degree in Computer Science, Informatics, Information systems or similar fields.\\nKey responsibilities include:\\n· Designing and building scalable data pipelines to extract, transform, and load data from a variety of sources.\\n· Maintaining and optimizing existing data pipelines and automating data workflows such as data ingestion, aggregation, and ETL processing.\\n· Working with data analysts in Fintech space to understand data needs and design appropriate data models.\\n· Collaborating with cross-functional teams to integrate data pipelines with other systems.\\n· Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures.\\n· Monitor data systems performance and implement optimization strategies.\\nMinimum Qualifications:\\n· Strong development experience in at least two or more of the following languages: Python, Java, Scala, Go.\\n· Experience with working on large data sets and distributed computing in Enterprise using technologies like Hadoop, Spark, Hive, Kafka, etc.\\n· Proficient with building data pipelines and workflow management tools. Should have worked on Airflow, Nifi or related tools.\\n· Working knowledge of message queuing, stream processing systems like Kafka, Spark Streaming.\\n· Hands on experience working with NoSQL databases (like Cassandra, MongoDB, or Neo4j) and relational databases (like MySQL, PostgreSQL, or Oracle)\\n· Experience with data visualization tools like Tableau, Superset, or similar tools.\\n· Working knowledge of data quality, data governance, and data security principles.\\nGood to have:\\n· Experience with data science and machine learning tools and libraries (e.g. scikit-learn, TensorFlow, PyTorch, etc.)\\n· Cloud computing experience (e.g. AWS, GCP, Azure)\\n· Cloudera Certified Professional Data Engineer, IBM Certified Data Engineer, or similar certifications.\\n· Must be aware of data privacy and security concerns and regulations (e.g. GDPR, CCPA, etc.)\\n Required Skills: , Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases', start_char_idx=None, end_char_idx=None, text_template='Metadata: {metadata_str}\\n-----\\nJob Listing: {content}', metadata_template='{key}->{value}', metadata_seperator='\\n'), score=0.3400910461794048),\n",
       " NodeWithScore(node=TextNode(id_='ac1f6838-5efe-4c43-af99-74880c0ef285', embedding=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-finergic-solutions-873f05c6b856dd6cd4aad3d66081217c', 'job_title': 'Data Engineer', 'company': 'FINERGIC SOLUTIONS PTE. LTD.', 'salary_min': 4000, 'salary_max': 6000, 'skills': 'Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases'}, excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'], excluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0ff1ab08-b7e5-434c-98a4-59d67b1b2ef2', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-finergic-solutions-873f05c6b856dd6cd4aad3d66081217c', 'job_title': 'Data Engineer', 'company': 'FINERGIC SOLUTIONS PTE. LTD.', 'salary_min': 4000, 'salary_max': 6000, 'skills': 'Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases'}, hash='5e2172e388b8f8723435c7df6206bd6b97054c9f01eaf996787ce060a9285cc9')}, hash='5a7e002bdc68ad8939f7283f6e28244626a1bcc3bfbc865728a651664b783e16', text='Who we are\\nWe are a boutique management consulting firm offering Business Consulting, Advisory, and Technology services to our banking clients in the wealth space. We partner with our clients on large transformation projects and act as their delivery partners.\\nWe are building a competency center with a high-potential team with niche skill sets. Our focus is to bring together, a diverse talent pool that will create perfect synergy in delivering the best possible solutions for the projects we undertake. We also aspire to provide an inclusive workplace where fresh talent can grow and achieve their full potential.\\nJob Description\\nThe candidate must have 5+ years of experience in a Data Engineer role with a degree in Computer Science, Informatics, Information systems or similar fields.\\nKey responsibilities include:\\n· Designing and building scalable data pipelines to extract, transform, and load data from a variety of sources.\\n· Maintaining and optimizing existing data pipelines and automating data workflows such as data ingestion, aggregation, and ETL processing.\\n· Working with data analysts in Fintech space to understand data needs and design appropriate data models.\\n· Collaborating with cross-functional teams to integrate data pipelines with other systems.\\n· Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures.\\n· Monitor data systems performance and implement optimization strategies.\\nMinimum Qualifications:\\n· Strong development experience in at least two or more of the following languages: Python, Java, Scala, Go.\\n· Experience with working on large data sets and distributed computing in Enterprise using technologies like Hadoop, Spark, Hive, Kafka, etc.\\n· Proficient with building data pipelines and workflow management tools. Should have worked on Airflow, Nifi or related tools.\\n· Working knowledge of message queuing, stream processing systems like Kafka, Spark Streaming.\\n· Hands on experience working with NoSQL databases (like Cassandra, MongoDB, or Neo4j) and relational databases (like MySQL, PostgreSQL, or Oracle)\\n· Experience with data visualization tools like Tableau, Superset, or similar tools.\\n· Working knowledge of data quality, data governance, and data security principles.\\nGood to have:\\n· Experience with data science and machine learning tools and libraries (e.g. scikit-learn, TensorFlow, PyTorch, etc.)\\n· Cloud computing experience (e.g. AWS, GCP, Azure)\\n· Cloudera Certified Professional Data Engineer, IBM Certified Data Engineer, or similar certifications.\\n· Must be aware of data privacy and security concerns and regulations (e.g. GDPR, CCPA, etc.)\\n Required Skills: , Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases', start_char_idx=None, end_char_idx=None, text_template='Metadata: {metadata_str}\\n-----\\nJob Listing: {content}', metadata_template='{key}->{value}', metadata_seperator='\\n'), score=0.3400910461794048),\n",
       " NodeWithScore(node=TextNode(id_='88b76686-6334-4c52-9ef7-cba5613189c7', embedding=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-finergic-solutions-873f05c6b856dd6cd4aad3d66081217c', 'job_title': 'Data Engineer', 'company': 'FINERGIC SOLUTIONS PTE. LTD.', 'salary_min': 4000, 'salary_max': 6000, 'skills': 'Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases'}, excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'], excluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='55f9423f-419f-4968-86f3-33ce387b5244', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-finergic-solutions-873f05c6b856dd6cd4aad3d66081217c', 'job_title': 'Data Engineer', 'company': 'FINERGIC SOLUTIONS PTE. LTD.', 'salary_min': 4000, 'salary_max': 6000, 'skills': 'Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases'}, hash='5e2172e388b8f8723435c7df6206bd6b97054c9f01eaf996787ce060a9285cc9')}, hash='5a7e002bdc68ad8939f7283f6e28244626a1bcc3bfbc865728a651664b783e16', text='Who we are\\nWe are a boutique management consulting firm offering Business Consulting, Advisory, and Technology services to our banking clients in the wealth space. We partner with our clients on large transformation projects and act as their delivery partners.\\nWe are building a competency center with a high-potential team with niche skill sets. Our focus is to bring together, a diverse talent pool that will create perfect synergy in delivering the best possible solutions for the projects we undertake. We also aspire to provide an inclusive workplace where fresh talent can grow and achieve their full potential.\\nJob Description\\nThe candidate must have 5+ years of experience in a Data Engineer role with a degree in Computer Science, Informatics, Information systems or similar fields.\\nKey responsibilities include:\\n· Designing and building scalable data pipelines to extract, transform, and load data from a variety of sources.\\n· Maintaining and optimizing existing data pipelines and automating data workflows such as data ingestion, aggregation, and ETL processing.\\n· Working with data analysts in Fintech space to understand data needs and design appropriate data models.\\n· Collaborating with cross-functional teams to integrate data pipelines with other systems.\\n· Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures.\\n· Monitor data systems performance and implement optimization strategies.\\nMinimum Qualifications:\\n· Strong development experience in at least two or more of the following languages: Python, Java, Scala, Go.\\n· Experience with working on large data sets and distributed computing in Enterprise using technologies like Hadoop, Spark, Hive, Kafka, etc.\\n· Proficient with building data pipelines and workflow management tools. Should have worked on Airflow, Nifi or related tools.\\n· Working knowledge of message queuing, stream processing systems like Kafka, Spark Streaming.\\n· Hands on experience working with NoSQL databases (like Cassandra, MongoDB, or Neo4j) and relational databases (like MySQL, PostgreSQL, or Oracle)\\n· Experience with data visualization tools like Tableau, Superset, or similar tools.\\n· Working knowledge of data quality, data governance, and data security principles.\\nGood to have:\\n· Experience with data science and machine learning tools and libraries (e.g. scikit-learn, TensorFlow, PyTorch, etc.)\\n· Cloud computing experience (e.g. AWS, GCP, Azure)\\n· Cloudera Certified Professional Data Engineer, IBM Certified Data Engineer, or similar certifications.\\n· Must be aware of data privacy and security concerns and regulations (e.g. GDPR, CCPA, etc.)\\n Required Skills: , Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases', start_char_idx=None, end_char_idx=None, text_template='Metadata: {metadata_str}\\n-----\\nJob Listing: {content}', metadata_template='{key}->{value}', metadata_seperator='\\n'), score=0.3400909785130692),\n",
       " NodeWithScore(node=TextNode(id_='4119340b-c17e-4c6d-9601-27b4602c9843', embedding=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-finergic-solutions-873f05c6b856dd6cd4aad3d66081217c', 'job_title': 'Data Engineer', 'company': 'FINERGIC SOLUTIONS PTE. LTD.', 'salary_min': 4000, 'salary_max': 6000, 'skills': 'Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases'}, excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'], excluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fcef1c6c-1be8-4723-a08e-d86500e3e758', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-finergic-solutions-873f05c6b856dd6cd4aad3d66081217c', 'job_title': 'Data Engineer', 'company': 'FINERGIC SOLUTIONS PTE. LTD.', 'salary_min': 4000, 'salary_max': 6000, 'skills': 'Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases'}, hash='5e2172e388b8f8723435c7df6206bd6b97054c9f01eaf996787ce060a9285cc9')}, hash='5a7e002bdc68ad8939f7283f6e28244626a1bcc3bfbc865728a651664b783e16', text='Who we are\\nWe are a boutique management consulting firm offering Business Consulting, Advisory, and Technology services to our banking clients in the wealth space. We partner with our clients on large transformation projects and act as their delivery partners.\\nWe are building a competency center with a high-potential team with niche skill sets. Our focus is to bring together, a diverse talent pool that will create perfect synergy in delivering the best possible solutions for the projects we undertake. We also aspire to provide an inclusive workplace where fresh talent can grow and achieve their full potential.\\nJob Description\\nThe candidate must have 5+ years of experience in a Data Engineer role with a degree in Computer Science, Informatics, Information systems or similar fields.\\nKey responsibilities include:\\n· Designing and building scalable data pipelines to extract, transform, and load data from a variety of sources.\\n· Maintaining and optimizing existing data pipelines and automating data workflows such as data ingestion, aggregation, and ETL processing.\\n· Working with data analysts in Fintech space to understand data needs and design appropriate data models.\\n· Collaborating with cross-functional teams to integrate data pipelines with other systems.\\n· Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures.\\n· Monitor data systems performance and implement optimization strategies.\\nMinimum Qualifications:\\n· Strong development experience in at least two or more of the following languages: Python, Java, Scala, Go.\\n· Experience with working on large data sets and distributed computing in Enterprise using technologies like Hadoop, Spark, Hive, Kafka, etc.\\n· Proficient with building data pipelines and workflow management tools. Should have worked on Airflow, Nifi or related tools.\\n· Working knowledge of message queuing, stream processing systems like Kafka, Spark Streaming.\\n· Hands on experience working with NoSQL databases (like Cassandra, MongoDB, or Neo4j) and relational databases (like MySQL, PostgreSQL, or Oracle)\\n· Experience with data visualization tools like Tableau, Superset, or similar tools.\\n· Working knowledge of data quality, data governance, and data security principles.\\nGood to have:\\n· Experience with data science and machine learning tools and libraries (e.g. scikit-learn, TensorFlow, PyTorch, etc.)\\n· Cloud computing experience (e.g. AWS, GCP, Azure)\\n· Cloudera Certified Professional Data Engineer, IBM Certified Data Engineer, or similar certifications.\\n· Must be aware of data privacy and security concerns and regulations (e.g. GDPR, CCPA, etc.)\\n Required Skills: , Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases', start_char_idx=None, end_char_idx=None, text_template='Metadata: {metadata_str}\\n-----\\nJob Listing: {content}', metadata_template='{key}->{value}', metadata_seperator='\\n'), score=0.3400909785130692),\n",
       " NodeWithScore(node=TextNode(id_='e573769c-4ab4-4478-b476-2c2c0dea0f93', embedding=None, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-finergic-solutions-873f05c6b856dd6cd4aad3d66081217c', 'job_title': 'Data Engineer', 'company': 'FINERGIC SOLUTIONS PTE. LTD.', 'salary_min': 4000, 'salary_max': 6000, 'skills': 'Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases'}, excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'], excluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d2ab0215-a4a0-4f15-afb8-74fe5eb4b090', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-finergic-solutions-873f05c6b856dd6cd4aad3d66081217c', 'job_title': 'Data Engineer', 'company': 'FINERGIC SOLUTIONS PTE. LTD.', 'salary_min': 4000, 'salary_max': 6000, 'skills': 'Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases'}, hash='5e2172e388b8f8723435c7df6206bd6b97054c9f01eaf996787ce060a9285cc9')}, hash='5a7e002bdc68ad8939f7283f6e28244626a1bcc3bfbc865728a651664b783e16', text='Who we are\\nWe are a boutique management consulting firm offering Business Consulting, Advisory, and Technology services to our banking clients in the wealth space. We partner with our clients on large transformation projects and act as their delivery partners.\\nWe are building a competency center with a high-potential team with niche skill sets. Our focus is to bring together, a diverse talent pool that will create perfect synergy in delivering the best possible solutions for the projects we undertake. We also aspire to provide an inclusive workplace where fresh talent can grow and achieve their full potential.\\nJob Description\\nThe candidate must have 5+ years of experience in a Data Engineer role with a degree in Computer Science, Informatics, Information systems or similar fields.\\nKey responsibilities include:\\n· Designing and building scalable data pipelines to extract, transform, and load data from a variety of sources.\\n· Maintaining and optimizing existing data pipelines and automating data workflows such as data ingestion, aggregation, and ETL processing.\\n· Working with data analysts in Fintech space to understand data needs and design appropriate data models.\\n· Collaborating with cross-functional teams to integrate data pipelines with other systems.\\n· Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures.\\n· Monitor data systems performance and implement optimization strategies.\\nMinimum Qualifications:\\n· Strong development experience in at least two or more of the following languages: Python, Java, Scala, Go.\\n· Experience with working on large data sets and distributed computing in Enterprise using technologies like Hadoop, Spark, Hive, Kafka, etc.\\n· Proficient with building data pipelines and workflow management tools. Should have worked on Airflow, Nifi or related tools.\\n· Working knowledge of message queuing, stream processing systems like Kafka, Spark Streaming.\\n· Hands on experience working with NoSQL databases (like Cassandra, MongoDB, or Neo4j) and relational databases (like MySQL, PostgreSQL, or Oracle)\\n· Experience with data visualization tools like Tableau, Superset, or similar tools.\\n· Working knowledge of data quality, data governance, and data security principles.\\nGood to have:\\n· Experience with data science and machine learning tools and libraries (e.g. scikit-learn, TensorFlow, PyTorch, etc.)\\n· Cloud computing experience (e.g. AWS, GCP, Azure)\\n· Cloudera Certified Professional Data Engineer, IBM Certified Data Engineer, or similar certifications.\\n· Must be aware of data privacy and security concerns and regulations (e.g. GDPR, CCPA, etc.)\\n Required Skills: , Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases', start_char_idx=None, end_char_idx=None, text_template='Metadata: {metadata_str}\\n-----\\nJob Listing: {content}', metadata_template='{key}->{value}', metadata_seperator='\\n'), score=0.3400909785130692)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.retrievers import VectorIndexRetriever\n",
    "# from llama_index import RetrieverQueryEngine, RagResponseSynthesizer\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# from llama_index import ResponseSynthesizer, ServiceContext\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "def setup_environment():\n",
    "    # Load environment variables from .env\n",
    "    load_dotenv()\n",
    "    # os.environ['OPENAI_API_KEY'] = 'sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "    os.environ[\"LLAMA_INDEX_CACHE_DIR\"] = \"cache\"\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "def create_document_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    resume = Document(\n",
    "        text=content,\n",
    "        metadata={\n",
    "            'file_path': file_path,\n",
    "        },\n",
    "        excluded_llm_metadata_keys=['file_path'],\n",
    "        excluded_embed_metadata_keys=['file_path'],\n",
    "        metadata_separator=\"::\",\n",
    "        metadata_template=\"{key}->{value}\",\n",
    "        text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n",
    "    )\n",
    "\n",
    "    return resume\n",
    "\n",
    "def load_documents():\n",
    "    # return SimpleDirectoryReader(\n",
    "    #     path,\n",
    "    #     recursive=True,\n",
    "    #     required_exts=[\".pdf\"],\n",
    "    #     filename_as_id=True,\n",
    "    # ).load_data()\n",
    "\n",
    "    JSON_LOAD_FILE = \"./jobslist.json\"\n",
    "    SLEEP_DELAY = 0.5\n",
    "\n",
    "    mcf_listings = MyCareersFutureListings(sleep_delay=SLEEP_DELAY)\n",
    "    listings = mcf_listings.load_json(json_load_file=JSON_LOAD_FILE)\n",
    "\n",
    "    reduced = []\n",
    "    for listing in listings:\n",
    "        reduced.append(\n",
    "            {\n",
    "                \"url\": listing[\"metadata\"][\"jobDetailsUrl\"],\n",
    "                \"job_title\": listing[\"title\"],\n",
    "                \"job_desc\": listing[\"job_desc\"],\n",
    "                \"company\": listing[\"postedCompany\"][\"name\"],\n",
    "                \"salary_min\": listing[\"salary\"][\"minimum\"],\n",
    "                \"salary_max\": listing[\"salary\"][\"maximum\"],\n",
    "                \"skills\": \", \".join([skill[\"skill\"] for skill in listing[\"skills\"]]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    documents = [\n",
    "        Document(\n",
    "            text=listing[\"job_desc\"],\n",
    "            metadata={\n",
    "                \"url\": listing[\"url\"],\n",
    "                \"job_title\": listing[\"job_title\"],\n",
    "                \"company\": listing[\"company\"],\n",
    "                \"salary_min\": listing[\"salary_min\"],\n",
    "                \"salary_max\": listing[\"salary_max\"],\n",
    "                \"skills\": listing[\"skills\"],\n",
    "            },\n",
    "            excluded_llm_metadata_keys=[\"url\", \"salary_min\", \"salary_max\"],\n",
    "            excluded_embed_metadata_keys=[\"url\", \"salary_min\", \"salary_max\"],\n",
    "            metadata_separator=\"::\",\n",
    "            metadata_template=\"{key}->{value}\",\n",
    "            text_template=\"Metadata: {metadata_str}\\n-----\\nJob Listing: {content}\",\n",
    "        )\n",
    "        for listing in reduced\n",
    "    ]\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def setup_index(documents):\n",
    "    db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    embed_model = LangchainEmbedding(\n",
    "        HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    )\n",
    "    service_context_embedding = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "    return VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        service_context=service_context_embedding,\n",
    "    )\n",
    "\n",
    "\n",
    "def setup_query_engine(index):\n",
    "    # node_parser = SimpleNodeParser.from_defaults(\n",
    "    #     text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "    # )\n",
    "    # prompt_helper = PromptHelper(\n",
    "    #     context_window=4096,\n",
    "    #     num_output=256,\n",
    "    #     chunk_overlap_ratio=0.1,\n",
    "    #     chunk_size_limit=None,\n",
    "    # )\n",
    "\n",
    "    # service_context = ServiceContext.from_defaults(\n",
    "    #     llm=llm,\n",
    "    #     embed_model=embed_model,\n",
    "    #     node_parser=node_parser,\n",
    "    #     prompt_helper=prompt_helper,\n",
    "    # )\n",
    "    service_context_llm = ServiceContext.from_defaults(\n",
    "        llm=OpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.1,\n",
    "        ),\n",
    "        # system_prompt=\"You are an AI assistant assisting job seekers find the best matches based on their profile.\"\n",
    "    )\n",
    "    # retriever = VectorIndexRetriever(\n",
    "    #     index=index,\n",
    "    #     similarity_top_k=10,\n",
    "    # )\n",
    "    retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        response_mode=\"compact\",\n",
    "        service_context=service_context_llm,\n",
    "        use_async=False,\n",
    "        streaming=False,\n",
    "    )\n",
    "\n",
    "    query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever=retriever, response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    return query_engine\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/resume.txt', 'r') as file:\n",
    "    content = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger = get_logger()\n",
    "    setup_environment()\n",
    "\n",
    "    documents = load_documents()\n",
    "    index = setup_index(documents)\n",
    "    query_engine = setup_query_engine(index)\n",
    "\n",
    "    user_input = f\"Which of the retrieved job listings best match the user's resume below? Return the company name for the top 3 best fits.\\n\\n RESUME:\\n{content}\\n\"\n",
    "    result = query_engine.query(user_input)\n",
    "    print(f\"Answer: {str(result)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "setup_environment()\n",
    "\n",
    "documents = load_documents()\n",
    "index = setup_index(documents)\n",
    "query_engine = setup_query_engine(index)\n",
    "\n",
    "user_input = f\"Which of the retrieved job listings best match the user's resume below? Return the company name for the top 3 best fits.\\n\\n RESUME:\\n{content}\\n\"\n",
    "result = query_engine.query(user_input)\n",
    "print(f\"Answer: {str(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by loading your data in the form of Document objects. LlamaIndex has several data loaders which can help you load Documents via the load_data method.\n",
    "\n",
    "from llama_index import Document\n",
    "\n",
    "# data_loader = DataLoader('path_to_your_data')\n",
    "# documents = data_loader.load_data()\n",
    "\n",
    "\n",
    "\n",
    "documents = [Document(\n",
    "\ttext=listing['job_desc'],\n",
    "\tmetadata={\n",
    "     'url' : listing['url'],\n",
    "     'job_title' : listing['job_title'],\n",
    "     'company' : listing['company'],\n",
    "     'salary_min' : listing['salary_min'],\n",
    "     'salary_max' : listing['salary_max'],\n",
    "     'skills' : listing['skills'],\n",
    "           },\n",
    "\texcluded_llm_metadata_keys=['url', 'salary_min', 'salary_max'],\n",
    "    excluded_embed_metadata_keys=['url', 'salary_min', 'salary_max'],\n",
    "\tmetadata_separator=\"::\",\n",
    "\tmetadata_template=\"{key}->{value}\",\n",
    "\ttext_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n",
    ") for listing in reduced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ServiceContext in LlamaIndex is a utility container that bundles commonly used resources during the indexing and querying stages of a LlamaIndex pipeline or application.\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.embeddings import BertEmbedding\n",
    "\n",
    "embed_model = BertEmbedding()\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "\n",
    "# In the above example, we are using BertEmbedding for vector embeddings. Replace BertEmbedding with your chosen model if needed.\n",
    "\n",
    "# You can create the index using the documents loaded earlier and the service context.\n",
    "\n",
    "\n",
    "from llama_index import LlamaIndex\n",
    "\n",
    "index = LlamaIndex(documents=documents, service_context=service_context)\n",
    "index.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# You can now use the AutoMergingRetriever to fetch relevant context from the index given a user query.\n",
    "\n",
    "from llama_index import AutoMergingRetriever\n",
    "\n",
    "retriever = AutoMergingRetriever(index)\n",
    "\n",
    "# Finally, create the RetrieverQueryEngine using the retriever and a response synthesizer. The retriever fetches relevant IndexNode objects from the index, and the response synthesizer is used to generate a natural language response based on the retrieved nodes and the user query.\n",
    "\n",
    "from llama_index import RetrieverQueryEngine, RagResponseSynthesizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from llama_index import ResponseSynthesizer, ServiceContext\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"your_model_id\"  # Replace with the ID of your desired Huggingface model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "\n",
    "response_synthesizer = ResponseSynthesizer(model=model, tokenizer=tokenizer)\n",
    "service_context = ServiceContext.from_defaults(response_synthesizer=response_synthesizer)\n",
    "\n",
    "\n",
    "# response_synthesizer = RagResponseSynthesizer('path_to_llama_2.0_model')\n",
    "query_engine = RetrieverQueryEngine(retriever, response_synthesizer)\n",
    "\n",
    "response = query_engine.query('Your query here')\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "   model_id = \"your_model_id\"  # Replace with the ID of your desired Huggingface model\n",
    "   tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "   model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\n",
    "\n",
    "   from llama_index import OptimumEmbedding, ServiceContext\n",
    "\n",
    "   embed_model = OptimumEmbedding(model=model, tokenizer=tokenizer)\n",
    "   service_context = ServiceContext.from_defaults(embed_model=embed_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
