{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "from src.mycareersfuture import MyCareersFutureListings\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# change these\n",
    "data = {\n",
    "    \"sessionId\": \"\",\n",
    "    \"search\": \"data\",\n",
    "    \"salary\": 6000,\n",
    "    \"positionLevels\": [\"Executive\", \"Junior Executive\", \"Fresh/entry level\"],\n",
    "    \"postingCompany\": []\n",
    "}\n",
    "\n",
    "start_url = \"https://api.mycareersfuture.gov.sg/v2/search?limit=20&page=0\"\n",
    "\n",
    "json_save_file = \"./jobslist.json\"\n",
    "\n",
    "SLEEP_DELAY = 0.5 # secs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the scrape and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'jobPostId': 'MCF-2023-0802627', 'updatedAt': '2023-10-20T08:15:38', 'newPostingDate': '2023-10-20', 'totalNumberJobApplication': 0, 'isPostedOnBehalf': False, 'isHideSalary': False, 'isHideHiringEmployerName': False, 'jobDetailsUrl': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd'}, 'hiringCompany': None, 'address': {'overseasCountry': None, 'foreignAddress1': None, 'foreignAddress2': None, 'block': '3', 'street': 'SHENTON WAY', 'floor': None, 'unit': None, 'building': 'SHENTON HOUSE', 'postalCode': '068805', 'isOverseas': False, 'districts': [{'id': 1, 'location': 'D01 Cecil, Marina, People’s Park, Raffles Place', 'region': 'Central', 'sectors': ['01', '02', '03', '04', '05', '06'], 'regionId': 'Central'}], 'lat': 1.27854545481515, 'lng': 103.850090052093}, 'positionLevels': [{'id': 9, 'position': 'Executive'}], 'schemes': [], 'postedCompany': {'uen': '202228958E', 'name': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'logoFileName': 'cae3af0d358237da8a66cc55f641af42/lyneer-corp.jpg', 'logoUploadPath': 'https://static.mycareersfuture.gov.sg/images/company/logos/cae3af0d358237da8a66cc55f641af42/lyneer-corp.jpg'}, 'salary': {'minimum': 5000, 'maximum': 7000, 'type': {'salaryType': 'Monthly'}}, 'skills': [{'skill': 'Airflow', 'uuid': '2528f15a184a14d75d2e092e396dd03e'}, {'skill': 'Data Analysis', 'uuid': '323d6a1348df47d3174f578e1eff00eb'}, {'skill': 'Big Data', 'uuid': '3a9c1c8521b12754fc7582be72383e28'}, {'skill': 'Hadoop', 'uuid': '53eb3dcfbb4c210bcd4fe1a985d7c946'}, {'skill': 'Agile', 'uuid': '657fd3eec6624217c3ad5de3744a69d2'}, {'skill': 'Data Integration', 'uuid': '6d8ab8489413756b1af6bdab6b80863e'}, {'skill': 'Data Quality', 'uuid': '74c365aff7ca1dee7d22bdad58071514'}, {'skill': 'Data Governance', 'uuid': '7c8d5f6eb1e6bf93b12b639d08b57ce9'}, {'skill': 'MapReduce', 'uuid': '7eeb3ba8fe5357a3041e66d311d3d28b'}, {'skill': 'Data Design', 'uuid': '7fc8a78ebe3c826cc2ca3e7083c48bfd'}, {'skill': 'Tuning', 'uuid': '89dd2948ecda7325c0001d8a0e51a2d8'}, {'skill': 'SQL', 'uuid': '9778840a0100cb30c982876741b0b5a2'}, {'skill': 'Debugging', 'uuid': 'da7d763aac39b294c7453c77c10d8239'}, {'skill': 'Software Development', 'uuid': 'f287f429e4026672e5754c1201691e49'}], 'categories': [{'id': 21, 'category': 'Information Technology'}], 'employmentTypes': [{'id': 3, 'employmentType': 'Contract'}, {'id': 8, 'employmentType': 'Full Time'}], 'shiftPattern': None, 'uuid': 'a2cc2d749776ff08d1df8ac0d7c2ecfd', 'title': 'Data Analyst/Data Engineer', 'status': {'id': '102', 'jobStatus': 'Open'}, 'score': None, 'job_desc': 'Job Description:\\n\\n  Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\\n\\nRequired and desired skills/qualifications:\\n\\n  Desired candidate should be having around 3-5 years of experience.\\n  Have strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\\n  Experience in monitoring, Tuning tasks on Cloudera distribution.\\n  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\\n  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\\n  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\\n  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\\n  Good experience working with different Hadoop file formats like Sequence File, ORC, AVRO and Parquet.\\n  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\\n  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\\n  In-depth understanding of Data Structure and Algorithms.\\n  Implemented in setting up standards and processes for Hadoop based application design and implementation.\\n  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\\n  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\\n  Good experience in CI/CD pipeline and working in Agile environment.\\n  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\\n  Developed analytical components using SparkSql and Spark Stream.\\n  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\\n  Good knowledge streaming data using Kafka from multiple sources into HDFS.\\n  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\\n  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications\\n\\n Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here and load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'jobPostId': 'MCF-2023-0802627', 'updatedAt': '2023-10-20T08:15:38', 'newPostingDate': '2023-10-20', 'totalNumberJobApplication': 0, 'isPostedOnBehalf': False, 'isHideSalary': False, 'isHideHiringEmployerName': False, 'jobDetailsUrl': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd'}, 'hiringCompany': None, 'address': {'overseasCountry': None, 'foreignAddress1': None, 'foreignAddress2': None, 'block': '3', 'street': 'SHENTON WAY', 'floor': None, 'unit': None, 'building': 'SHENTON HOUSE', 'postalCode': '068805', 'isOverseas': False, 'districts': [{'id': 1, 'location': 'D01 Cecil, Marina, People’s Park, Raffles Place', 'region': 'Central', 'sectors': ['01', '02', '03', '04', '05', '06'], 'regionId': 'Central'}], 'lat': 1.27854545481515, 'lng': 103.850090052093}, 'positionLevels': [{'id': 9, 'position': 'Executive'}], 'schemes': [], 'postedCompany': {'uen': '202228958E', 'name': 'LYNEER CORP (SINGAPORE) PTE. LTD.', 'logoFileName': 'cae3af0d358237da8a66cc55f641af42/lyneer-corp.jpg', 'logoUploadPath': 'https://static.mycareersfuture.gov.sg/images/company/logos/cae3af0d358237da8a66cc55f641af42/lyneer-corp.jpg'}, 'salary': {'minimum': 5000, 'maximum': 7000, 'type': {'salaryType': 'Monthly'}}, 'skills': [{'skill': 'Airflow', 'uuid': '2528f15a184a14d75d2e092e396dd03e'}, {'skill': 'Data Analysis', 'uuid': '323d6a1348df47d3174f578e1eff00eb'}, {'skill': 'Big Data', 'uuid': '3a9c1c8521b12754fc7582be72383e28'}, {'skill': 'Hadoop', 'uuid': '53eb3dcfbb4c210bcd4fe1a985d7c946'}, {'skill': 'Agile', 'uuid': '657fd3eec6624217c3ad5de3744a69d2'}, {'skill': 'Data Integration', 'uuid': '6d8ab8489413756b1af6bdab6b80863e'}, {'skill': 'Data Quality', 'uuid': '74c365aff7ca1dee7d22bdad58071514'}, {'skill': 'Data Governance', 'uuid': '7c8d5f6eb1e6bf93b12b639d08b57ce9'}, {'skill': 'MapReduce', 'uuid': '7eeb3ba8fe5357a3041e66d311d3d28b'}, {'skill': 'Data Design', 'uuid': '7fc8a78ebe3c826cc2ca3e7083c48bfd'}, {'skill': 'Tuning', 'uuid': '89dd2948ecda7325c0001d8a0e51a2d8'}, {'skill': 'SQL', 'uuid': '9778840a0100cb30c982876741b0b5a2'}, {'skill': 'Debugging', 'uuid': 'da7d763aac39b294c7453c77c10d8239'}, {'skill': 'Software Development', 'uuid': 'f287f429e4026672e5754c1201691e49'}], 'categories': [{'id': 21, 'category': 'Information Technology'}], 'employmentTypes': [{'id': 3, 'employmentType': 'Contract'}, {'id': 8, 'employmentType': 'Full Time'}], 'shiftPattern': None, 'uuid': 'a2cc2d749776ff08d1df8ac0d7c2ecfd', 'title': 'Data Analyst/Data Engineer', 'status': {'id': '102', 'jobStatus': 'Open'}, 'score': None, 'job_desc': 'Job Description:\\n\\n  Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\\n\\nRequired and desired skills/qualifications:\\n\\n  Desired candidate should be having around 3-5 years of experience.\\n  Have strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\\n  Experience in monitoring, Tuning tasks on Cloudera distribution.\\n  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\\n  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\\n  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\\n  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\\n  Good experience working with different Hadoop file formats like Sequence File, ORC, AVRO and Parquet.\\n  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\\n  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\\n  In-depth understanding of Data Structure and Algorithms.\\n  Implemented in setting up standards and processes for Hadoop based application design and implementation.\\n  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\\n  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\\n  Good experience in CI/CD pipeline and working in Agile environment.\\n  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\\n  Developed analytical components using SparkSql and Spark Stream.\\n  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\\n  Good knowledge streaming data using Kafka from multiple sources into HDFS.\\n  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\\n  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications\\n\\n Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'}\n"
     ]
    }
   ],
   "source": [
    "lst = MyCareersFutureListings(sleep_delay=SLEEP_DELAY)\n",
    "listings = lst.load_json(json_load_file=json_save_file)\n",
    "\n",
    "print(listings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnecessary fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`listings` is still a lot of metadata, still deciding what fields relevant to reduce it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-analystdata-engineer-lyneer-corp-a2cc2d749776ff08d1df8ac0d7c2ecfd',\n",
       "  'job_title': 'Data Analyst/Data Engineer',\n",
       "  'job_desc': 'Job Description:\\n\\n  Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\\n\\nRequired and desired skills/qualifications:\\n\\n  Desired candidate should be having around 3-5 years of experience.\\n  Have strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\\n  Experience in monitoring, Tuning tasks on Cloudera distribution.\\n  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\\n  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\\n  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\\n  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\\n  Good experience working with different Hadoop file formats like Sequence File, ORC, AVRO and Parquet.\\n  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\\n  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\\n  In-depth understanding of Data Structure and Algorithms.\\n  Implemented in setting up standards and processes for Hadoop based application design and implementation.\\n  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\\n  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\\n  Good experience in CI/CD pipeline and working in Agile environment.\\n  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\\n  Developed analytical components using SparkSql and Spark Stream.\\n  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\\n  Good knowledge streaming data using Kafka from multiple sources into HDFS.\\n  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\\n  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications\\n\\n Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development',\n",
       "  'company': 'LYNEER CORP (SINGAPORE) PTE. LTD.',\n",
       "  'salary_min': 5000,\n",
       "  'salary_max': 7000,\n",
       "  'skills': 'Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'},\n",
       " {'url': 'https://www.mycareersfuture.gov.sg/job/building-construction/data-engineer-sunray-woodcraft-construction-6dd6985a33109bca5ee16f6d6849d8f4',\n",
       "  'job_title': 'Data Engineer',\n",
       "  'job_desc': 'Job Description\\n●      Data Pipeline Construction: Design, construct, install, test, and maintain highly scalable data management systems. This includes building and maintaining robust, error-free data pipelines.\\n●      Data Flow Optimization: Work to enhance data flow and collection to improve data reliability, efficiency, and quality.\\n●      Machine Learning Pipeline: Construct and maintain machine learning pipelines, ensuring they meet organization needs and adhere to necessary standards.\\n●      Big Data Handling: Implement systems and tools to deal with big data. Leverage systems such as Kafka, Spark, and Flink for big data processing.\\n●      Cloud Computing: Use cloud resources effectively to streamline data processing, especially Azure Cloud Services.\\n●      Dashboarding: Build interactive, insightful dashboards to communicate data and results to stakeholders.\\n●      Team Collaboration: Collaborate with data scientists and architects on several projects. Translate complex functional and technical requirements into detailed design.\\n●      Documentation: Create and maintain optimal data pipeline architecture and create data tools for analytics and data scientist team members.\\n Required Skills: , Machine Learning, Team Collaboration, Construction, Azure, Big Data, Cloud Computing, Pipelines, Hadoop, Data Management, Architects, ETL, Reliability, Data Engineering, Cloud Services, Databases',\n",
       "  'company': 'SUNRAY WOODCRAFT CONSTRUCTION PTE LTD',\n",
       "  'salary_min': 5000,\n",
       "  'salary_max': 7000,\n",
       "  'skills': 'Machine Learning, Team Collaboration, Construction, Azure, Big Data, Cloud Computing, Pipelines, Hadoop, Data Management, Architects, ETL, Reliability, Data Engineering, Cloud Services, Databases'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced = []\n",
    "for listing in listings:\n",
    "    reduced.append({\n",
    "        'url' : listing['metadata']['jobDetailsUrl'],\n",
    "        'job_title' : listing['title'],\n",
    "        'job_desc' : listing['job_desc'],\n",
    "        'company' : listing['postedCompany']['name'],\n",
    "        'salary_min' : listing['salary']['minimum'],\n",
    "        'salary_max' : listing['salary']['maximum'],\n",
    "        'skills' : ', '.join([skill['skill'] for skill in listing['skills']]),\n",
    "    })\n",
    "\n",
    "reduced[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description:\n",
      "\n",
      "  Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\n",
      "\n",
      "Required and desired skills/qualifications:\n",
      "\n",
      "  Desired candidate should be having around 3-5 years of experience.\n",
      "  Have strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\n",
      "  Experience in monitoring, Tuning tasks on Cloudera distribution.\n",
      "  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\n",
      "  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\n",
      "  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\n",
      "  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\n",
      "  Good experience working with different Hadoop file formats like Sequence File, ORC, AVRO and Parquet.\n",
      "  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\n",
      "  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\n",
      "  In-depth understanding of Data Structure and Algorithms.\n",
      "  Implemented in setting up standards and processes for Hadoop based application design and implementation.\n",
      "  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\n",
      "  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\n",
      "  Good experience in CI/CD pipeline and working in Agile environment.\n",
      "  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\n",
      "  Developed analytical components using SparkSql and Spark Stream.\n",
      "  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\n",
      "  Good knowledge streaming data using Kafka from multiple sources into HDFS.\n",
      "  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\n",
      "  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications\n",
      "\n",
      " Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Description\n",
      "●      Data Pipeline Construction: Design, construct, install, test, and maintain highly scalable data management systems. This includes building and maintaining robust, error-free data pipelines.\n",
      "●      Data Flow Optimization: Work to enhance data flow and collection to improve data reliability, efficiency, and quality.\n",
      "●      Machine Learning Pipeline: Construct and maintain machine learning pipelines, ensuring they meet organization needs and adhere to necessary standards.\n",
      "●      Big Data Handling: Implement systems and tools to deal with big data. Leverage systems such as Kafka, Spark, and Flink for big data processing.\n",
      "●      Cloud Computing: Use cloud resources effectively to streamline data processing, especially Azure Cloud Services.\n",
      "●      Dashboarding: Build interactive, insightful dashboards to communicate data and results to stakeholders.\n",
      "●      Team Collaboration: Collaborate with data scientists and architects on several projects. Translate complex functional and technical requirements into detailed design.\n",
      "●      Documentation: Create and maintain optimal data pipeline architecture and create data tools for analytics and data scientist team members.\n",
      " Required Skills: , Machine Learning, Team Collaboration, Construction, Azure, Big Data, Cloud Computing, Pipelines, Hadoop, Data Management, Architects, ETL, Reliability, Data Engineering, Cloud Services, Databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What To Expect\n",
      "\n",
      "  Translating business requirements into technical specifications, including data streams, integrations, transformations, databases, data warehouses and Big-data • Defining the data architecture framework, standards, and principles, including modeling, metadata, security\n",
      "  Implementation and guide best approach for the implementation of CDC, HA, Backup Recovery, and Audit policies\n",
      "  Defining reference architecture, which is a pattern that others can follow to create and improve data systems\n",
      "  Collaborating and coordinating with multiple departments, stakeholders, partners, and external vendors\n",
      "  Design and implement effective database solutions and models to store and retrieve company data\n",
      "  Examine and identify data structural necessities by evaluating applications and programming\n",
      "  Assess data implementation procedures to ensure they comply with internal and external regulations\n",
      "  Prepare accurate database design and architecture reports for management and executive teams\n",
      "  Recommend solutions to improve new and existing database systems\n",
      "\n",
      "What You'll Bring\n",
      "\n",
      "  Bachelor's Degree in Computer Science, Industrial Engineering, Electrical Engineering • At least 5 years of experience in Data Architecture\n",
      "  Experience in Data Architecture Design, Data Modeling and Data Integration\n",
      "  Experience in Data Governance and Security, Data Storage and Management\n",
      "  Experience in Performance Optimization\n",
      "  Development Skills (JAVA, C, C++, C#), Database(RDBMS, Big Data)\n",
      "\n",
      " Required Skills: , Modeling, Big Data, Data Modeling, Translating, Industrial Engineering, Data Integration, Data Governance, Architecture Design, Data Architecture, Database Design, Metadata, Java, Electrical Engineering, Databases, C++, Business Requirements\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Responsibilities:\n",
      "• Gather, assess, refine, and translate business requirements into technology requirements.\n",
      "• Design data architecture framework and standards for scalability, performance, high availability, and reliability throughout the data life cycle. \n",
      "• Work closely with project team (business, solutioning and infrastructure, etc) throughout the project cycle.\n",
      "• Define and govern data architecture artifacts.\n",
      "Requirements:\n",
      "• Bachelor of Science degree in Computer Science or related discipline.\n",
      "• Minimum 3 years of experience in designing enterprise data platform.\n",
      "• Minimum 5 years working experience in implementing data related projects.\n",
      "• Experience in working on cloudnative data platforms, such as Microsoft Azure Cloud, etc.\n",
      "• Experience in Data Lake and Data Warehouse design and best practices.\n",
      "• Experience in building ETL/ELT data pipelines including the best practices, such as data cleansing, data tokenisation and ensuring data quality.\n",
      "• Fluency in statistical software (e.g., Python, R, Scala), database languages (e.g., SQL).\n",
      " Required Skills: , Microsoft Azure, Big Data, Solutioning, Pipelines, Interpersonal Skills, Hadoop, Data Management, ETL, Data Quality, Data Governance, SQL, Python, Data Architecture, Data Warehousing, Business Requirements\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Our client, one of the leading organisations in Asia-Pacific is looking for:\n",
      "Data Engineer\n",
      "Responsibilities:\n",
      "\n",
      "  Design, Architect, Deploy, and maintain solutions on AWS and Databricks to provide secure and governed access to data for data scientist, data analysts and business users.\n",
      "  Manage the full life-cycle of a data lakehouse project from requirement gathering to data modelling, design of the data architecture and deployment.\n",
      "  Collaborate with data stewards, data analysts and data scientists to build data pipelines to ingest data from enterprise systems for both batch and real-time streaming data.\n",
      "  Establish and manage the complete machine learning lifecycle using MLFlow.\n",
      "\n",
      "Requirements:\n",
      "\n",
      "  Minimum 2 to 3 years of relevant work experience.\n",
      "  Degree in Computer Science or Information Technology or related disciplines\n",
      "  Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.\n",
      "  Understanding on OLTP, Data Lake and Lakehouse technologies that may include knowledge of S3, AWS Glue, DeltaLake or DataBricks\n",
      "  Proficient in SQL, PySpark and Python.\n",
      "  Experience in Big Data management &amp; processing using tools such as Spark\n",
      "  Knowledge in Machine Learning Frameworks and the use of ML Flow to manage the machine learning lifecycle\n",
      "  Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.\n",
      "  Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.\n",
      "  Familiar in building REST services is good to have.\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Interested applicants can also email CV at vimmi@nsearchglobal.com (for faster processing, please state the exact job/position title applied “Data Engineer”)\n",
      "Only shortlisted candidates will be notified.\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "EA License Number: 10C3636\n",
      "EA Personnel Name: Vimmi Baunthiyal\n",
      "EA Personnel Registration Number: R1543982\n",
      " Required Skills: , Machine Learning, Git, PySpark, Kubernetes, Azure, Big Data, Pipelines, Architect, Hadoop, Data Management, Data Engineering, SQL, Python, Data Architecture, Docker, S3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Our client is embarking on an exciting multi-year program, encompassing the creation of multi cloud-native projects that stand as integral components of their Business &amp; Digital Transformation initiative. This endeavor, facilitated by strategic partnerships with leading software developers, seeks to leverage agile methodologies, microservices architecture, and Azure DevOps to establish our client as a digital frontrunner in their industry. As we embark on this journey, we invite individuals who aspire to shape their careers alongside us, fostering growth within a dynamic and entrepreneurial culture. Collaborate with global experts and gain access to cutting-edge technologies in a role that fuels ambition.\n",
      "In the capacity of a Data Engineer, you will contribute to project implementation by collecting, aggregating, storing, and facilitating accessible data from diverse sources for analysis and decision-making. This role's significance extends to the data supply chain, ensuring stakeholders can easily access and manipulate data for routine and ad hoc analysis. Your involvement spans the entire data lifecycle, encompassing data ingestion, analytics, and actionable insights.\n",
      "Responsibilities\n",
      "· Translate intricate business requirements into technical solutions, harnessing your robust business acumen.\n",
      "· Evaluate existing business procedures and practices while identifying forthcoming business prospects that can leverage Microsoft Azure Data &amp; Analytics PaaS Services.\n",
      "· Contribute to the planning and execution of data design services, offering guidance on sizing, configuration, and conducting need assessments.\n",
      "· Champion the creation of architectures that transform and modernize enterprise data solutions through Azure cloud data technologies.\n",
      "· Design and construct contemporary Data Pipelines, Data Streams and Data Service APIs.\n",
      "· Develop and manage data warehouse schematics, layouts, architectures, and both relational and non-relational databases for data accessibility and advanced analytics.\n",
      "· Facilitate data exposure to end-users using platforms like PowerBI, Azure API Apps, or other modern visualization tools.\n",
      "· Implement metrics and monitoring protocols that yield effective results.\n",
      "Requirements\n",
      "· Bachelor’s degree in computer science /Computer Engineering, or equivalent.\n",
      "· Agile, Azure/MS Certifications.\n",
      "· A proven track record of 3 to 5 years in the Data Engineer domain.\n",
      "· Demonstrated expertise in translating business use cases and requirements into effective technical solutions.\n",
      "· Experience in mapping data and analytics solutions within business processes.\n",
      "· Proficiency in applying methods that address business challenges by utilizing one or more Azure Data and Analytics services, complemented by data pipeline construction, data stream integration, and system harmony.\n",
      "· Proficiency in various programming languages and data manipulation tools to ensure data integrity, quality, and efficiency.\n",
      "· Proficiency in Cloud Data Transformation using tools such as Azure Data Factory, Azure Synapse Analytics, and SSIS.\n",
      "· Proficiency in composing articulate SQL scripts and Stored Procedures to achieve desired data transformations.\n",
      "· Competence in tools like SSMS and strong aptitude in Python.\n",
      "· Familiarity with DevOps processes including CI/CD and Infrastructure as Code fundamentals.\n",
      "· Familiarity with Data Governance tools like Microsoft Purview, Master Data Management (MDM), and Data Quality processes.\n",
      "· Essential knowledge of Power BI and Embedded Power BI is necessary.\n",
      "· Proficiency in Azure Data Lake and Azure SQL/SQL MI is mandatory. Additional experience with Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, and Azure Stream Analytics is a plus.\n",
      "· Understanding of data modeling, database management, performance optimization, and integration with other applications.\n",
      "· Preferred experience in preparing data for Data Science and Machine Learning applications; experience with Azure Machine Learning / Azure Databricks is advantageous.\n",
      "· Effective collaboration within cross-functional teams, including remote collaborations.\n",
      "· Proficiency in Visual Studio, PowerShell Scripting, and ARM templates.\n",
      "Others\n",
      "· MNC, good corporate culture and 5-day work week (Town area).\n",
      "· With AWS and variable bonus.\n",
      "To apply, please send your CV to talentagent@innergy-consulting.com\n",
      "We regret that only shortlisted candidates will be notified.\n",
      " Required Skills: , Machine Learning, Microsoft Azure, Factory, Azure, Data Modeling, Pipelines, ARM, Data Transformation, Scripting, Data Quality, SQL, Python, Data Science, API, Power BI, Databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Who we are\n",
      "We are a boutique management consulting firm offering Business Consulting, Advisory, and Technology services to our banking clients in the wealth space. We partner with our clients on large transformation projects and act as their delivery partners.\n",
      "We are building a competency center with a high-potential team with niche skill sets. Our focus is to bring together, a diverse talent pool that will create perfect synergy in delivering the best possible solutions for the projects we undertake. We also aspire to provide an inclusive workplace where fresh talent can grow and achieve their full potential.\n",
      "Job Description\n",
      "The candidate must have 5+ years of experience in a Data Engineer role with a degree in Computer Science, Informatics, Information systems or similar fields.\n",
      "Key responsibilities include:\n",
      "· Designing and building scalable data pipelines to extract, transform, and load data from a variety of sources.\n",
      "· Maintaining and optimizing existing data pipelines and automating data workflows such as data ingestion, aggregation, and ETL processing.\n",
      "· Working with data analysts in Fintech space to understand data needs and design appropriate data models.\n",
      "· Collaborating with cross-functional teams to integrate data pipelines with other systems.\n",
      "· Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures.\n",
      "· Monitor data systems performance and implement optimization strategies.\n",
      "Minimum Qualifications:\n",
      "· Strong development experience in at least two or more of the following languages: Python, Java, Scala, Go.\n",
      "· Experience with working on large data sets and distributed computing in Enterprise using technologies like Hadoop, Spark, Hive, Kafka, etc.\n",
      "· Proficient with building data pipelines and workflow management tools. Should have worked on Airflow, Nifi or related tools.\n",
      "· Working knowledge of message queuing, stream processing systems like Kafka, Spark Streaming.\n",
      "· Hands on experience working with NoSQL databases (like Cassandra, MongoDB, or Neo4j) and relational databases (like MySQL, PostgreSQL, or Oracle)\n",
      "· Experience with data visualization tools like Tableau, Superset, or similar tools.\n",
      "· Working knowledge of data quality, data governance, and data security principles.\n",
      "Good to have:\n",
      "· Experience with data science and machine learning tools and libraries (e.g. scikit-learn, TensorFlow, PyTorch, etc.)\n",
      "· Cloud computing experience (e.g. AWS, GCP, Azure)\n",
      "· Cloudera Certified Professional Data Engineer, IBM Certified Data Engineer, or similar certifications.\n",
      "· Must be aware of data privacy and security concerns and regulations (e.g. GDPR, CCPA, etc.)\n",
      " Required Skills: , Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Description\n",
      "As a data engineer, you are responsible for designing, developing, testing, and maintaining ETL (extract-transform-load) programs. We are using these programs every day to handle a large amount of data coming from our products worldwide. Your work will influence users of our products as well as internal staff from other departments. You will work collaboratively with other colleagues to make decisions on technologies and product features.\n",
      "\n",
      "  Collect data from various data sources, clean and ingest them into data warehouse.\n",
      "  Design data warehouse models and ETL pipelines. Develop, deliver and maintain data jobs using various technologies.\n",
      "  Deep dive into data jobs with performance issues, figure out the root cause, and optimize them accordingly.\n",
      "  Collaborate with data analysts and external users, understand and normalize their use cases on each data service, build and maintain development frameworks accordingly.\n",
      "  Collaborate with data platform engineers, build and maintain data engine of platform features.\n",
      "  Manage data, scripts and documentation in accordance with compliance rules.\n",
      "\n",
      "Job Requirements\n",
      "\n",
      "  Bachelor's degree or higher in Computer Science, Business Analytics, Mathematics, or related disciplines.\n",
      "  Self-learner with a strong sense of ownership.\n",
      "  Familiar with one of the following programming languages: Python, Scala, Java.\n",
      "  Experience with one or more of the following: Spark, Hadoop, Oozie, MySQL, Flink, Kafka, Airflow.\n",
      "  Experience with Linux and Shell scripting.\n",
      "  Passionate about programming and new technologies in the field.\n",
      "  Strong written and verbal communication skills.\n",
      "  Strong interpersonal skills and a team player.\n",
      "\n",
      " Required Skills: , Analytical Skills, Critical Thinking, Shell Scripting, MySQL, Problem Solving, Project Management, Python, Java, Programming, Linux\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Description:\n",
      "\n",
      "Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\n",
      "\n",
      "Required and desired skills/qualifications:\n",
      "\n",
      "  At least 4 years experience is required.\n",
      "  Strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\n",
      "  Experience in monitoring, Tuning tasks on  Cloudera distribution.\n",
      "  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\n",
      "  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\n",
      "  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\n",
      "  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\n",
      "  Good experience working with different Hadoop file formats like Sequence File,  ORC, AVRO and Parquet.\n",
      "  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\n",
      "  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\n",
      "  In-depth understanding of Data Structure and Algorithms.\n",
      "  Implemented in setting up standards and processes for Hadoop based application design and implementation.\n",
      "  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\n",
      "  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\n",
      "  Good experience in CI/CD pipeline and working in Agile environment.\n",
      "  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\n",
      "  Developed analytical components using SparkSql and Spark Stream.\n",
      "  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\n",
      "  Good knowledge streaming data using Kafka from multiple sources into HDFS.\n",
      "  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\n",
      "  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications.\n",
      "\n",
      " Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, ETL, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We are seeking talents who are motivated and analytical to join our team as a Data Analyst. We are working on an enterprise level ML model among the top operators in healthcare and other industries. This job provides practical experience in data analysis, machine learning, algorithms in a nascent ML application.\n",
      "\n",
      "Primary Responsibility\n",
      "• Carry out data preparation using statistical and other methods.\n",
      "• Carry out machine learning projects in data training and inference.\n",
      "• Design and build analysis model to generate training and hypothesis testing.\n",
      "• Collate and manage project results for follow-on projects development.\n",
      "• Review ML and algorithmic models from technical journals. Provide pros/cons analysis to shortlist for feasibility testing.\n",
      "• Perform data analytics on datasets for preparation and data insights for ML tuning.\n",
      "\n",
      "Requirement\n",
      "• Degree in Data Science, Physics, Statistics, Mathematics, or Computer Science.\n",
      "• Strong analytical and problem-solving skills.\n",
      "• Competent in Python data analysis, ML tools etc.\n",
      "• Familiar with various ML framework and algorithmic programming methods.\n",
      "• New graduates is welcome but must have practical experience in data analytics and ML.\n",
      "\n",
      "Benefits\n",
      "• Work in a dynamic pace and product rich environment.\n",
      "• Able to work with and experience best-of-breed software design and implement experts.\n",
      "• Abundance of deep-tech learning experience.\n",
      "• Progression between Data Analyst and Data Scientist roles.\n",
      "• Actual Renumeration depends on experience.\n",
      " Required Skills: , Tableau, Machine Learning, Microsoft Excel, Data Analysis, Healthcare, Physics, Mathematics, Tuning, SQL, Python, Software Design, Statistics, Data Science, Data Analytics, Databases, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction:\n",
      "BlueSG is the first electric car sharing service in Singapore. We rely on data to drive business decisions and optimize our operations to maintain a high level of ridership, cost efficiency and customer service. The role of the data analyst is critical in helping us to derive relevant insights from our data to meet these goals.\n",
      "You will be a part of the in-house data team, working side by side with a high-performance group of data scientists and data engineers.\n",
      "BlueSG is currently building our next generation data architecture in-house on the AWS platform. This presents rare and valuable learning opportunities for the data analyst.\n",
      "Responsibilities:\n",
      "\n",
      "  You will be a part of the data team working closely with business teams, including product and marketing, to:\n",
      "    \n",
      "      Formulate meaningful quantified metrics to track success of initiatives and campaigns.\n",
      "      Design and implement dashboards and reports to visualize these metrics and support business decisions.\n",
      "      Perform analysis to understand correlation / causality between business data and business outcomes, operations data and processes, to uncover insights that can help us improve our bottom and top line.\n",
      "      Deep dive into our customer database and transaction activities, to better understand our customer groups and provide insights that will help us serve them better.\n",
      "      Design and run experiments including A/B tests to quantify the cost-benefit of campaigns / initiatives, define thresholds for success and understand the longer-term value of such campaigns / initiatives.\n",
      "    \n",
      "  \n",
      "  Within the data team, you will work with and provide inputs to our data engineers to create the data models and data pipelines to support our data analytics work.\n",
      "\n",
      "Requirements:\n",
      "\n",
      "  Bachelor in Statistics, Mathematics, Analytics, Computer Science or related discipline.\n",
      "  Proficient in analytical tools, especially:\n",
      "    \n",
      "      Python’s Pandas and SQL for data manipulation;\n",
      "      Matplotlib or similar tools for plotting;\n",
      "      Power BI, Metabase, or similar tools for dashboards and reports.\n",
      "    \n",
      "  \n",
      "  Familiar with standard statistical methods such as time series, hypothesis testing, and how we can apply them to business problems.\n",
      "  Familiar in using Git and Github for version control and collaboration.\n",
      "  Comfortable using Jira and Confluence for task tracking, documentation and knowledge sharing.\n",
      "  Analytical, attention to detail.\n",
      "  Strong communications skills to understand requirements from and share findings and insights with business departments and senior management.\n",
      "\n",
      "Good to have:\n",
      "\n",
      "  Preferably 2 years of experience in data analytics.\n",
      "\n",
      " Required Skills: , Tableau, Version Control, Git, Confluence, Pandas, Pipelines, Mathematics, SQL, JIRA, Attention to Detail, Data Architecture, Statistics, Data Analytics, Github, Power BI, Matplotlib\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*Annual renewable contract, full time\n",
      "*Tanjong Pagar\n",
      "\n",
      "Key Responsibilities:\n",
      "\n",
      "  Work closely with the data engineering team to develop, maintain, and optimize data pipelines.\n",
      "  Write efficient Python and SQL code to extract, transform, and load data from various sources into our data ecosystem.\n",
      "  Assist in the deployment and management of data solutions within containerization platforms.\n",
      "  Collaborate with the broader development team on data-related components of C# applications.\n",
      "  Contribute to database design and maintenance, ensuring data integrity, performance, and security.\n",
      "  Identify opportunities for process improvements and optimization of data workflows.\n",
      "\n",
      "Qualifications:\n",
      "\n",
      "  Proficiency in Python and SQL, with the ability to write clean and efficient code for data extraction and transformation.\n",
      "  A good understanding of containerization platforms, such as Docker or Kubernetes.\n",
      "  Sound knowledge of database concepts, including data modeling, SQL databases, and NoSQL databases.\n",
      "  Experience in C# development is a plus.\n",
      "  Strong problem-solving skills and attention to detail.\n",
      "  Eagerness to learn and adapt to new technologies and tools in the data engineering field.\n",
      "\n",
      "How to Apply:\n",
      "If you're interested, please submit your application through this job posting or you may submit your updated resume to kailun@dhc.com.sg. Unfortunately, only candidates who are selected for further consideration will be notified.\n",
      "\n",
      "Ng Kai Lun\n",
      "Registration number R22105927 | EA License: 12C6253\n",
      "DYNAMIC HUMAN CAPITAL PTE. LTD. | 2 Kallang Avenue #03-08 CT Hub Singapore 339407\n",
      " Required Skills: , Kubernetes, Big Data, Data Modeling, Pipelines, Hadoop, ETL, Data Engineering, SQL, Attention to Detail, Python, Containerization, Database Design, Docker, C#, Databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Analyst\n",
      "1 year contract, potential to renew\n",
      "Come and join us!\n",
      "Do you wish to work in a world-class organization trying your hands at something you have dreamt of doing?\n",
      "“FIND YOUR PLACE” by joining a world-class US Consumer company \n",
      "Get an opportunity to explore new technology, learn new skills, enjoy the diverse and open culture, engagement and care, flexible working model, career opportunities, competitive salary and bonus, and endless amenities and benefits.\n",
      "Company Description:\n",
      "Our client is an American consumer health company. It is the proprietor of well-known consumer brands. They combine the power of science with meaningful human insights to empower people around the world to live healthier lives\n",
      "Key Responsibilities\n",
      "\n",
      "  Work with data analytics lead to support Regional Plan Excellence team on data analytics requirements. \n",
      "  Develop and prepare dashboards and data models to provide business intelligence insights to the team based on requirements. \n",
      "  Implement change requests on existing data analytics solutions such as enhancements on dashboards, reports and data workflows.  \n",
      "  Work with a data operations team on report generation and weekly/monthly automated data flows.\n",
      "  Analyse with data to answer business questions and present findings and recommendations to managers.\n",
      "  Keep up to date with the company’s suite of digital tools.\n",
      "  Reconcile data files and identify discrepancies and variances.\n",
      "\n",
      "Qualifications, Relevant Skills, Knowledge, and Experience\n",
      "\n",
      "  Bachelor’s degree in data science, information systems, computer science or equivalent degrees. \n",
      "  2-5 years of experience working with data such as identifying insights using data, developing reports and analytical solutions, communicating insights to stakeholders. \n",
      "  Strong proficiency in data analytics tools or data mining software\n",
      "  Data visualization: Power BI/Power Query, Tableau\n",
      "  Data modelling: SQL (required), Alteryx\n",
      "  Proficiency in MS Office (Excel, Word, PowerPoint). \n",
      "  Good to Have: Supply chain / logistics background.\n",
      "  Strong attention to detail and ability to notice discrepancies in data.\n",
      "  Self-starter with the ability to set and meet objectives and work independently.\n",
      "  Interest in learning about data analytics and discovering ways to create competitive edge in business performance.\n",
      "\n",
      "Benefits\n",
      "\n",
      "  Exposure to real business problems, be part of a team creating tangible solutions tackling and solving real business problems.\n",
      "  Become knowledgeable and educated on a wide range of data analytics use cases in supply chain and logistics.\n",
      "\n",
      "Applicants must be fully vaccinated or have a valid exemption in accordance with MOM’s regulations to allow them to enter the workplace. Applicants may be required to share verifiable COVID-19 vaccination documents or proof of a valid exemption at the point of offer. Randstad Pte. Limited and/or the Client reserves the right to withdraw an offer if the applicant fails to provide verifiable COVID-19 vaccination and/or proof of exemption documents.\n",
      "Interested parties, please apply through this link https://jnj-apac.talent-pool.com/projects OR click on APPLY button. Alternatively, you can share your CV at joleyn.chin@randstadsourceright.com.sg\n",
      "EA License: 94C3609 \n",
      "Reg No: R1440247\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Required Skills: , Tableau, Business Intelligence, Microsoft Excel, Reserves, Data Analysis, Strong Attention To Detail, Supply Chain, Alteryx, Data Mining, SQL, Statistics, Data Science, Data Analytics, Working Model, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Responsibilities\n",
      "1. Responsible for daily forecasting and analyzing of the e-commerce platform operations. You will also be required to generate operation analysis reports, provide conclusions and suggestions to various departments.\n",
      "2. Build a data analysis model, provide data support for daily operation activities, and be able to provide strategies and suggestions for business development based on data analysis.\n",
      "3. Develop and improve A/B Test plans for different business strategies, summarize and analyze, continuously optimize, and ensure that they bring actual value to business growth.\n",
      "4. Independently undertake data operation analysis work, provide data decision-making and strategic support for business and products, the analysis direction includes but not limited to relevant analysis such as activity effects, growth conversion, user behavior, and membership stratification.\n",
      "\n",
      "Job Requirements:\n",
      "1. More than 1 years of data analysis experience, major in statistics, e-commerce, etc., rich experience in data analysis and data products in the e-commerce industry.\n",
      "2. Able to conduct good analysis and possess excellent data report presentation capabilities, able to think and analyse problems systematically. Meticulous and possess strong execution ability.\n",
      "3. Familiar with e-commerce-related data products, understand e-commerce-related dimensions of indicators and improvement methods.\n",
      "4. Familiar with the operation of e-commerce. retail, FMCG, or fast fashion industries.\n",
      " Required Skills: , Tableau, Machine Learning, Forecasting, Microsoft Excel, Data Analysis, Mathematics, FMCG, SQL, Python, Statistics, Business Development, Data Analytics, Power BI, Databases, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We are hiring for a Data Analyst (business) with the investment management division of a global insurance company.\n",
      "\n",
      "Job Title: Data Analyst\n",
      "\n",
      "Role: This Data Analyst position involves primarily assist the team with data and business-related tasks, it is not necessary an investment related position.\n",
      "\n",
      "  Extract, integrate, and analyze diverse data sources, including structured and unstructured data, social media, financial information, and more, to derive actionable insights for informed business decision-making.\n",
      "  Utilize data mining techniques to identify patterns, business trends, and opportunities for leveraging analytics.\n",
      "  Apply a range of analytical methods, from descriptive to prescriptive, to address intricate business issues.\n",
      "  Create user-friendly, self-service data visualization tools that empower business stakeholders to access data insights effortlessly.\n",
      "  Communicate analysis findings in a non-technical manner, offering actionable insights and conclusions to drive business actions.\n",
      "\n",
      "Requirement:\n",
      "\n",
      "  Bachelor's Degree in Finance or a related discipline.\n",
      "  3-8 years of broad experience of DA experience, financial knowledge not compulsory, junior candidates can be trained.\n",
      "  Strong proficiency in Excel, good level of programming experience in R or Python. Candidates with SQL, Power Query, and PowerBI or Tableau skills will be given preference.\n",
      "\n",
      " Required Skills: , Tableau, Machine Learning, Microsoft Excel, Data Analysis, R Programming, Mathematics, PowerBI, Data Mining, SQL, Python, Excel, Statistics, Visualization, Data Analytics, Power BI, Databases, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Company Description\n",
      "STACS (Hashstacs Pte Ltd) is Asia’s leading ESG data and technology company, headquartered in Singapore. Its ESGpedia platform powers the Monetary Authority of Singapore’s (MAS) Greenprint ESG Registry and ASEAN Single Accesspoint for ESG Data (SAFE) Initiative. ESGpedia serves as the Nexus of ESG Finance with ESG data across multiple industries, as well as digital solutions for the financial sector to scale decarbonisation financing, and corporates and SMEs to attain their ESG goals. Empowering industries across Asia towards Net Zero, its clients and partners include global financial institutions, corporates, and SMEs.\n",
      "\n",
      "STACS is an Award Winner of the MAS Global FinTech Innovation Challenge Awards 2020, and a two-time awardee of the Financial Sector Technology and Innovation (FSTI) Proof of Concept (POC) grant, under the Financial Sector Development Fund administered by the MAS. STACS is also an IMDA Spark company.\n",
      "\n",
      "Job Summary\n",
      "We are looking for a Data Analyst with experience in handling large data sets and relational databases to contribute towards the development of our ESGpedia platform, a global ESG registry for the financial sector.\n",
      "The ideal candidate is a hands-on data analyst with a strong interest in data processing and visualization with strong familiarity in SQL and data cleaning tools. You must be comfortable working across multiple teams in a fast-paced environment with little supervision in an Agile environment. You will gain in depth knowledge of various ESG sectors that we work with and contribute to the data models as well as the end visual products that business users will utilize daily.\n",
      "\n",
      "Job Responsibilities\n",
      "Your core responsibilities:\n",
      "1. Define and implement data collection and validation logic for optimal scalability and performance with automation\n",
      "2. Work closely with the partnership team to understand analytical needs and deliver actionable insights to our end users\n",
      "3. Provide technical expertise in data storage structures, data mining and data cleaning\n",
      "4. Support initiatives for data integrity and normalization to improve internal data models\n",
      "5. Create and maintain interactive visualizations using cloud-based BI and ML tools\n",
      "\n",
      "Requirements\n",
      "· Strong technical writing and verbal communication skills\n",
      "· &gt;2 years of experience as a data analyst\n",
      "· Proven analytical skills such as data mining, evaluation, analysis and visualization\n",
      "· Demonstrated experience in handling data in relational and nosql databases\n",
      "· Proficient in SQL with a natural curiosity to pick up other analytic tools\n",
      "· Detail oriented and data driven, strong in problem solving skills\n",
      "· Basic coding knowledge, in any language (R, Javascript, Python) is preferred\n",
      "\n",
      "What you will need to thrive:\n",
      "· Value teamwork above all.\n",
      "· Natural curiosity to learn.\n",
      "· Loves to work with code.\n",
      "· Intrinsically motivated personality.\n",
      "· High adaptability and strong problem-solving skills.\n",
      "· Customer focused mindset.\n",
      "· Self-starter who thrives on complexity and independence.\n",
      "\n",
      "Notice\n",
      "We regret to inform that only shortlisted candidates will be notified. All applications will be treated with strictest confidence.\n",
      "By submitting any application or resume to us, you will be deemed to have read and agreed to the terms of our Privacy Policy(https://stacs.io/privacy-policy/), and consented to us collecting, using, retaining and disclosing your personal information to prospective employers for their consideration.\n",
      "You may refer and access our website(https://stacs.io/) for more information.\n",
      " Required Skills: , Analytical Skills, JavaScript, Problem Solving, NoSQL, Data Mining, SQL, Python, Evaluation, Communication Skills, Writing Skills\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Summary\n",
      "We're seeking a proactive Data Scientist to drive data-driven insights. Your role includes defining project goals, performing exploratory data analysis, selecting appropriate machine learning models, and communicating findings effectively. Collaboration with Data/ML engineers, infrastructure development, and staying updated with industry trends are key.\n",
      "\n",
      "Responsibilities\n",
      "· Work closely with stakeholders to define project objectives, gather requirements, and pinpoint data sources.\n",
      "\n",
      "  Perform exploratory data analysis, delivering iterative analyses to refine the problem at hand and explore potential solutions.\n",
      "  Choose and develop the most suitable machine learning algorithms or statistical models to address specific business  inquiries.\n",
      "  Interpret and convey analytical findings and insights to both technical and non-technical team members.\n",
      "  Collaborate with Data/ML engineers to seamlessly integrate models and algorithms into production systems.\n",
      "  Play a part in enhancing data science infrastructure, as well as creating tools and frameworks to facilitate efficient data analysis, experimentation, and model deployment.\n",
      "  Stay current with industry trends and identify best practices, incorporating them to foster excellence within the Data &amp; Analytics team.\n",
      "  Ensure that the delivery of solutions aligns with Agile methodologies.\n",
      "\n",
      "Requirements\n",
      "· Minimally 5 years experience as a Data Scientist with a proven track record of delivering impactful data-driven solutions.\n",
      "\n",
      "  Proficiency in Python and SQL, with demonstrated expertise in data manipulation and analysis using libraries such as Pandas, Polars, NumPy, and SciPy.\n",
      "  Strong command of machine learning frameworks and libraries, including PyTorch and scikit-learn, along with proficiency in data visualization tools like matplotlib and seaborn.\n",
      "  In-depth knowledge of machine learning algorithms, encompassing regression, SVM, gradient boosting, LSTNs,  Transformers, and other techniques, with a specific focus on working with time series datasets.\n",
      "  Familiarity with cloud platforms, particularly  AWS and Azure.\n",
      "  Exceptional problem-solving abilities,  underpinned by the capacity to think critically and analytically.\n",
      "  Effective communication skills, enabling the presentation of complex concepts and findings to both technical and non-technical stakeholders.\n",
      "  Self-motivated and a good team player.\n",
      "  Demonstrated domain expertise in commodities, spanning Sales, Trading, Risk, Supply Chain, Customer Interaction, and related areas, is highly preferred.\n",
      "  Strong background and experience in Natural  Language Processing and Large Language Models are highly advantageous.\n",
      "  Proficiency in implementing optimization methods,  including linear programming, is a valuable asset.\n",
      "  Familiarity with geospatial analysis is a beneficial addition to your skillset.\n",
      "\n",
      "We regret to inform that only shortlisted candidates will be notified / contacted.\n",
      "\n",
      "EA Registration No.: TAN XIN WEI, MARSHALL , R22109465\n",
      "Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544\n",
      " Required Skills: , Machine Learning, SciPy, Pandas, Data Analysis, Azure, Customer Interaction, Experimentation, Natural Language Processing, Agile Methodologies, PyTorch, SQL, Python, Data Science, Matplotlib, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Role and Responsibilities\n",
      "· Architect and/or review technical solutions for the deployment of AI models, along with the data pipelines and visualization interfaces\n",
      "· Utilize DevOps tools and automate model deployment / tuning via orchestration\n",
      "· Provide guidance for maintenance, support and enhancements to model deployment platforms\n",
      "· Manage MLOps Enginee whom will:\n",
      "· Analyze, design and develop test for model deployment and automation\n",
      "· Design and implement API interfaces\n",
      "· Set up Continuous   Integration / Continuous Deployment pipeline\n",
      "· Work with Data Scientists on understanding and enhancing data models for deployment\n",
      "· Liaise with Data Engineers on the requirements for each data sources, understand the ETL required\n",
      "· Work with DBA to manage the DB servers\n",
      " · Work with Cloud infra engineers for on-boarding to AWS and MS Azure\n",
      "Requirements / Qualifications\n",
      "· Practical experience of software engineering or software development experience in making AI/ML models   production ready\n",
      "· Degree and equivalent training (e.g. specialist diploma, professional certificate) in Business Analytics, Computer Science / Computer Engineering, Computer Engineering, Information Systems, Mathematics, Statistics, Engineering or related disciplines that possesses an analytical component\n",
      "· Understanding of programming language such as Python, R and SQL\n",
      "· Foundational knowledge of Cloud computing and infrastructure setup\n",
      "· Foundational knowledge of data pipeline, data engineering and data pre-processing\n",
      "· Foundational knowledge of data visualization tools such as Tableau, QlikSense, Power BI\n",
      "· Agile and Scrum experience is preferred\n",
      "· At least 4 - 6 years’ experience in developing, implementing and maintaining IT systems\n",
      " Required Skills: , Tableau, Azure, Pipelines, Mathematics, Software Engineering, ETL, Data Engineering, SQL, Python, Continuous Integration, Statistics, Visualization, Orchestration, API, Power BI, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Description\n",
      "As part of the Data Analytics team, you will be reporting to the Head of Big Data and Database. You will work with team members and line of businesses to develop and maintain comprehensive data pipeline architecture and analysis solutions to support business strategies.\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "  Create, maintain and improve optimal data pipeline architecture &amp; solution\n",
      "  Design, test, implement and manage datasets and processes\n",
      "  Work with business owners, data scientists and analysts to develop key business questions and build datasets that answer those questions\n",
      "  Evaluate &amp; ensure datasets for accuracy and quality\n",
      "  Handle various data requests\n",
      "  Ensure technical document and data dictionary are reliable and up-to-date\n",
      "  Explore new technologies especially Open Source suitable for implementation\n",
      "\n",
      "\n",
      "Requirements\n",
      "\n",
      "  Degree in Computer Science or engineering\n",
      "  Strong programming skills with SQL, NoSQL, Python, Scala, Java &amp; C#/C++\n",
      "  Experience with big data tools: Hadoop, Spark, Kafka &amp; ETL\n",
      "  Analytical skills with structure and unstructured datasets\n",
      "  Able to learn new technologies\n",
      "  Able to understand business requirement and gain domain knowledge\n",
      "  Able to ensure the quality of systems delivered\n",
      "  Working experience in system analysis and design\n",
      "  Problem-solving skills\n",
      "  Accuracy and attention to detail\n",
      "  Sound communication skills\n",
      "  Sound time management and prioritizing tasks skills\n",
      "  Team player\n",
      "  Independent\n",
      "  Responsible and reliable\n",
      "\n",
      " Required Skills: , Scala, Analytical Skills, Big Data, Pipelines, Hadoop, ETL, Data Dictionary, Open Source, SQL, Attention to Detail, Python, Time Management, Java, Data Analytics, Databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Responsibilities:\n",
      "• Apply state of the art machine learning knowledge to identify solutions to problems.\n",
      "• Work with large data sets, integrate diverse data sources, data types and data structures into the solution.\n",
      "• Develop analytical approaches to meet business requirements; this involves translating requests into use cases, test cases, preparation of training data sets and iterative algorithm development.\n",
      "• Collaborate with various stakeholders on the formulation and application of new modelling solutions for a variety of related problems.\n",
      "• Engage in deep research and identify new predictive modelling techniques as appropriate for a specific solution.\n",
      "• Present research results and recommendations.\n",
      "• Performs exploratory data analysis to gauge the need for or appropriateness of advanced analytical methods.\n",
      "Requirements:\n",
      "• Bachelor, Masters, or PhD in Mathematics, Computer Science, Statistics, or related field.\n",
      "• At least 2 years of industry experience building machine learning models.\n",
      "• Excellent understanding of machine learning concepts and techniques.\n",
      "• Proficiency in working with multiple data science tools &amp; languages (e.g., Python, R, Pandas, Spark).\n",
      "• Proficient in one or more query languages (e.g., SQL, Hive, Pig).\n",
      " Required Skills: , Machine Learning, Pandas, Big Data, Data Structures, Mathematics, Translating, Formulation, Test Cases, Data Mining, SQL, Algorithm Development, Python, Statistics, Data Science, Business Requirements\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for listing in reduced:\n",
    "    print(listing['job_desc'])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job Description:\\n\\n  Provides development and analytical support on various wealth products to ensure project goals are met. Adaptable to internal frameworks. Participates in the rollout of company-wide pilot programs developed as a result of programmed models. Duties primarily include the regular use of discretion, independent judgment, the ability to communicate with multiple levels of management and the utilization of core PRIDE behaviors.\\n\\nRequired and desired skills/qualifications:\\n\\n  Desired candidate should be having around 3-5 years of experience.\\n  Have strong technical foundation with in-depth knowledge in Big Data Hadoop, Data Reporting, Data Design, Data Analysis, Data governance, Data integration and Data quality.\\n  Experience in monitoring, Tuning tasks on Cloudera distribution.\\n  Deep and extensive knowledge with HDFS, Spark, MapReduce, Hive, HBase, Sqoop, Yarn, Airflow.\\n  Thorough knowledge on Hadoop architecture and various components such as HDFS, Name Node, Data Node, Application Master, Resource Manager, Node Manager, Job Tracker, Task Tracker and MapReduce programming paradigm.\\n  Good understanding on Hadoop MR1 and MR2 (YARN) Architecture.\\n  Efficient in working with Hive data warehouse tool creating tables, data distributing by implementing Partitioning and Bucketing strategy, writing and optimizing the HiveQL queries.\\n  Good experience working with different Hadoop file formats like Sequence File, ORC, AVRO and Parquet.\\n  Experience in using modern Big-Data tools like SparkSQL to convert schema-less data into more structured files for further analysis. Experience in Spark Streaming to receive real time data and store the stream data into HDFS.\\n  Good Knowledge on Hadoop Cluster architecture and monitoring the cluster.\\n  In-depth understanding of Data Structure and Algorithms.\\n  Implemented in setting up standards and processes for Hadoop based application design and implementation.\\n  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa.\\n  Experience in working on Avaloq data processing. Participation in multiple Avaloq Core Banking Platform implementations in various business / technical streams\\n  Good experience in CI/CD pipeline and working in Agile environment.\\n  Hands on experience with Real time streaming using Kafka, Spark streaming into HDFS.\\n  Developed analytical components using SparkSql and Spark Stream.\\n  Involved in converting Hive/SQL queries into Spark transformations using Spark SQL using Scala.\\n  Good knowledge streaming data using Kafka from multiple sources into HDFS.\\n  Knowledge of processing and analyzing real-time data streams/flows using Kafka and HBase.\\n  Proficient in all phases of software development including design, configuration, testing, debugging, implementation, release, and support of large-scale, Bank platform applications\\n\\n Required Skills: , Airflow, Data Analysis, Big Data, Hadoop, Agile, Data Integration, Data Quality, Data Governance, MapReduce, Data Design, Tuning, SQL, Debugging, Software Development'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_resume = \"\"\"\n",
    "Hanafi Haffidz\n",
    "(Mohammad Hanafi Bin Md Haffidz)\n",
    "CAIE Associate AI Engineer (Certified)\n",
    "English, Malay (written, spoken) | Singaporean\n",
    "hanafi.haffidz@gmail.com | +65 9150 6451\n",
    "portfolio: gammaraysky.github.io\n",
    "SUMMARY\n",
    "Ex-filmmaker turned AI Engineer, with over ten years\n",
    "experience in media and advertising, both as a freelancer\n",
    "and company owner.\n",
    "Experience in data engineering, machine learning, MLOps,\n",
    "project management, design thinking, training, content\n",
    "development, storytelling and communication.\n",
    "EXPERIENCE\n",
    "AI Apprentice (apprenticeship programme)\n",
    "AI Singapore (Feb 2023 - Nov 2023)\n",
    "● Core developer on an automated speech recognition project, collaborating closely with principal investigator\n",
    "NTU Speechlab to develop a voice activity detection model for far-field dialogue recognition.\n",
    "● Collaborated with a multidisciplinary team, effectively communicating findings, insights, and technical\n",
    "concepts to both technical and non-technical stakeholders. Participated in sprint planning, sprint reviews,\n",
    "technical 'Brown Bag' sharing sessions, and code reviews.\n",
    "● Conducted exploratory data analysis and feature engineering on audio datasets, utilising a custom Gradio\n",
    "frontend. This involved visual clustering of speech and non-speech classes for far-field vs. near-field audio\n",
    "using t-SNE. Experimented with various features, including MFCC, mel-spectrograms, and Wav2Vec 2.0\n",
    "embeddings.\n",
    "● Authored a data engineering and orchestration pipeline for ETL processes on audio datasets. Pipeline\n",
    "encompassed cleaning, validation, format conversions, data chunking, sampling, augmentation, model\n",
    "training, and evaluation. Tools used included Kedro and Apache Airflow.\n",
    "● Conducted literature review of current state-of-the-art models and performed model selection and evaluation.\n",
    "Compared Wav2Vec 2.0, NeMo MarbleNet, PyAnNet, ZFF-VAD, and Robust VAD models with experiment\n",
    "tracking using MLFlow, Tensorboard, and Pytorch Lightning. Further fine-tuned the selected model through\n",
    "data augmentation and hyperparameter optimization using Optuna.\n",
    "● Implemented unit, integration, and endpoint testing, as well as CI/CD pipelines using Gitlab.\n",
    "● Developed a scalable model-serving application (can run locally or cloud deployed for larger workloads).\n",
    "Technologies used include FastAPI, Celery, RabbitMQ, Redis, Docker, and Kubernetes.\n",
    "● Mentored junior apprentices during their deep-skilling phase, focusing on computer vision and classical\n",
    "supervised learning topics.\n",
    "● Worked on several side projects, including:\n",
    "○ Examination question generation, both open-ended and multiple-choice questions (MCQ), utilising\n",
    "both OpenAI GPT API and open-source large language models.\n",
    "○ Implemented Retrieval Augmented Generation using a custom fine-tuned Llama 2.0 and LlamaIndex\n",
    "for a question-answering chatbot proof-of-concept.\n",
    "○ Developed a multimodal hate speech detection system for image and textual data using ImageBind.\n",
    "○ Worked on plant disease detection (Object Detection) using smaller CNN models designed for\n",
    "deployment on edge devices.\n",
    "Data Analyst/Research Coordinator, Special Projects (freelance/ad hoc)\n",
    "Singapore Association of Motion Pictures Professionals (2019 - 2023)\n",
    "● Coordinated focus group studies and townhall meet ups for media industry practitioners & companies to\n",
    "gather snapshot of media industry (film/TV/advertising).\n",
    "● Conduct exploratory data analysis, data cleaning, aggregation, visualisations and creations of reports.\n",
    "Assisted executive committee in mapping out roadmap and key objectives for association, and presentation of\n",
    "findings to IMDA.\n",
    "● Consulted on database selection, data gathering, and analytics-related decisions for the association's ongoing\n",
    "development of a specialised jobs portal platform exclusively for the local media industry. Acted as product\n",
    "owner, defining design goals, prioritising features, and working closely with the development team throughout\n",
    "the development of the jobs platform app.\n",
    "Web Designer/Webmaster/Copywriter (freelance/ad hoc)\n",
    "Singapore Association of Motion Pictures Professionals (2019 - 2020)\n",
    "● Designed and maintained association website (https://www.sampp.org.sg). Tech stack included Wordpress,\n",
    "jQuery, MySQL.\n",
    "● Planning and copywriting of content for microsite (https://www.sampp.org.sg/mph) to educate media\n",
    "practitioners on many issues ranging from contracts, legal rights and statuses as a freelancer, to available\n",
    "government grants or information on permits such as for filming with aerial unmanned aircraft. Also consulted\n",
    "with lawyers during this process to draft boilerplate legal contracts that are free to use/modify.\n",
    "● Setup & maintenance of domain, webhost accounts, linking of APIs (e.g. Google Drive, emails, with onsite\n",
    "newsletters/forms), scripted custom Wordpress plugins.\n",
    "Documentary Video Producer (freelance) (2017 - 2021)\n",
    "● Applied design thinking framework and worked closely with clients to scope out requirements and messaging.\n",
    "● Craft story/script, organise production timeline, budget, hire additional crew or on-camera talent.\n",
    "● Conduct and film interviews, B-roll footage, produce graphical assets. Edit content with several rounds of\n",
    "review with client, filming additional content as needed.\n",
    "● Clients included Facebook, Twitter, Cisco Systems, A*STAR, Ministry of Home Affairs, Channel News Asia,\n",
    "VICE\n",
    "● Highlights:\n",
    "○ Over 30 videos produced for Facebook APAC across Singapore, Malaysia, Japan, Hong Kong,\n",
    "Taiwan.\n",
    "○ Docu-series for CNA (\"Into the Vault\"), MHA (\"Frontliners\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reduced[0]['job_desc']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employing LLMs for Semantic Similarity/RAG/Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- need some experimentation to find best path. based on the following constraints:\n",
    "- each JD is probably 3-4 paragraphs of text\n",
    "- the user's resume they may wish to put in is also at least a 1 pager of text\n",
    "- do we try summarizing each of those first to attempt semantic similarity of the embeddings?\n",
    "  - but summarization quality also varies, some models i've tested, asking it to summarize only the skills required, just returned 'data engineer'\n",
    "- do we try to split every sentence, make a list of embeddings, and try to score every resume sentence to every JD sentence, and somehow only save maximum scores/similarities (this sounds complicated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flan T5 XXL : \"Extract the skills required for the below job description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (0.0.220)\n",
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/ad/b1/6bb5006471264b5d75fcf0e3d7ed8d0bfc4ec335e08e05abf5900c42aa43/langchain-0.0.325-py3-none-any.whl.metadata\n",
      "  Using cached langchain-0.0.325-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (3.6.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (0.0.53)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\han\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (1.10.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\han\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\han\\.conda\\envs\\py310\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Using cached langchain-0.0.325-py3-none-any.whl (1.9 MB)\n",
      "Installing collected packages: langchain\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.220\n",
      "    Uninstalling langchain-0.0.220:\n",
      "      Successfully uninstalled langchain-0.0.220\n",
      "Successfully installed langchain-0.0.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\han\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\han\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\han\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "DEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\han\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index 0.8.53.post3 requires nest-asyncio<2.0.0,>=1.5.8, but you have nest-asyncio 1.5.6 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\han\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\han\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\han\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Architecture, Docker, S3, Git, PySpark, Kubernetes\n",
      "Data Engineer\n",
      "Data Analysis, Catalogs, Data Management, Data Quality, SQL, SAP, Data Migration, Attention\n",
      "Data Engineer\n"
     ]
    }
   ],
   "source": [
    "# API_URL = \"https://api-inference.huggingface.co/models/togethercomputer/RedPajama-INCITE-Chat-3B-v1\"\n",
    "# headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-xxl\"\n",
    "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "for listing in reduced[:4]:\n",
    "    output = query({\n",
    "        \"inputs\": f\"Extract the skills required for the below job description: \\n{listing['job_desc']}\",\n",
    "    })\n",
    "\n",
    "    print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flan T5 XXL : \"Summarize the job skills requirements in 200 words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Engineer - Design, Architect, Deploy, and maintain solutions on AWS and\n",
      "Data Engineer - Microsoft Azure - Town Area - MNC, good corporate culture and 5-\n",
      "Data Analyst for a global mining company. In the capacity of a Data Analyst, your primary\n",
      "Data Engineer - Fintech - New York, NY - 5+ years of experience in\n"
     ]
    }
   ],
   "source": [
    "for listing in reduced[:4]:\n",
    "    output = query({\n",
    "        \"inputs\": f\"Summarize the job skills requirements in 200 words: \\n{listing['job_desc']}\",\n",
    "    })\n",
    "\n",
    "    print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
