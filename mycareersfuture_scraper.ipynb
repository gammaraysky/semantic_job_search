{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyCareersFuture Job Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Generating this is easy.\n",
    "- Go to [mycareersfuture.gov.sg](mycareersfuture.gov.sg) and apply a search using the specs you want. (e.g. min salary, full time/contract/etc...)\n",
    "- Open web developer tools and go to Network. Refresh the page. (These instructions for Firefox, but Chrome should be similar)\n",
    "- Find the row item that says `GET`, `api.mycareersfuture.gov.sg` `search?search=data&salary=...` (this being whatever you specced)\n",
    "- Right click, Copy Value, Copy URL parameters. Below was my example.\n",
    "- You could also copy as curl command, send to chatGPT and ask it to convert it for you for as a Python request.\n",
    "\n",
    "```\n",
    "      search=data\n",
    "      salary=6000\n",
    "      positionLevel=Executive\n",
    "      positionLevel=Junior%20Executive\n",
    "      positionLevel=Fresh%2Fentry%20level\n",
    "      sortBy=relevancy\n",
    "      page=0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# change these\n",
    "data = {\n",
    "    \"sessionId\": \"\",\n",
    "    \"search\": \"data\",\n",
    "    \"salary\": 6000,\n",
    "    \"positionLevels\": [\"Executive\", \"Junior Executive\", \"Fresh/entry level\"],\n",
    "    \"postingCompany\": []\n",
    "}\n",
    "\n",
    "start_url = \"https://api.mycareersfuture.gov.sg/v2/search?limit=20&page=0\"\n",
    "\n",
    "json_save_file = \"./jobslist.json\"\n",
    "\n",
    "SLEEP_DELAY = 0.5 # secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the scrape and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'jobPostId': 'MCF-2023-0769409', 'updatedAt': '2023-10-09T08:29:36', 'newPostingDate': '2023-10-09', 'totalNumberJobApplication': 0, 'isPostedOnBehalf': False, 'isHideSalary': False, 'isHideHiringEmployerName': False, 'jobDetailsUrl': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-nsearch-global-f9a24b54305ae4c1bb7bcf9974f4e44b'}, 'hiringCompany': None, 'address': {'overseasCountry': None, 'foreignAddress1': None, 'foreignAddress2': None, 'block': None, 'street': None, 'floor': None, 'unit': None, 'building': None, 'postalCode': None, 'isOverseas': False, 'districts': [{'id': 998, 'location': 'Islandwide', 'region': 'Islandwide', 'sectors': [], 'regionId': 'Islandwide'}]}, 'positionLevels': [{'id': 9, 'position': 'Executive'}], 'schemes': [], 'postedCompany': {'uen': '200805822M', 'name': 'NSEARCH GLOBAL PTE. LTD.', 'logoFileName': 'ef210e1b98ea89c1ccc396f7f36a493b/NSEARCH GLOBAL PTE. LTD..jpg', 'logoUploadPath': 'https://static.mycareersfuture.gov.sg/images/company/logos/ef210e1b98ea89c1ccc396f7f36a493b/NSEARCH%20GLOBAL%20PTE.%20LTD..jpg'}, 'salary': {'minimum': 7000, 'maximum': 10000, 'type': {'salaryType': 'Monthly'}}, 'skills': [{'skill': 'Machine Learning', 'uuid': '091fa9121c047db1dd48c3e2ab5f3c91'}, {'skill': 'Git', 'uuid': '0bcc70105ad279503e31fe7b3f47b665'}, {'skill': 'PySpark', 'uuid': '17e918efeeeb8f100c695e284d5c0a08'}, {'skill': 'Kubernetes', 'uuid': '30136395f01879792198317c11831ea4'}, {'skill': 'Azure', 'uuid': '3a580f142203677f1f0bc30898f63f53'}, {'skill': 'Big Data', 'uuid': '3a9c1c8521b12754fc7582be72383e28'}, {'skill': 'Pipelines', 'uuid': '414a8ddf993e2641bf90821f28fb0d9a'}, {'skill': 'Architect', 'uuid': '4bb5da084da7cbef67d5f21bcb933af7'}, {'skill': 'Hadoop', 'uuid': '53eb3dcfbb4c210bcd4fe1a985d7c946'}, {'skill': 'Data Management', 'uuid': '568483e9bd85504f3c9dcef24ecd3235'}, {'skill': 'Data Engineering', 'uuid': '8c3b0099eed5df08053dc956ae63ee43'}, {'skill': 'SQL', 'uuid': '9778840a0100cb30c982876741b0b5a2'}, {'skill': 'Python', 'uuid': 'a7f5f35426b927411fc9231b56382173'}, {'skill': 'Data Architecture', 'uuid': 'b2ae7e1f4d402f8338b1f5e26636171b'}, {'skill': 'Docker', 'uuid': 'c5fd214cdd0d2b3b4272e73b022ba5c2'}, {'skill': 'S3', 'uuid': 'e2ab7c65b21ed8cc1c3b642b5e36429e'}], 'categories': [{'id': 21, 'category': 'Information Technology'}], 'employmentTypes': [{'id': 3, 'employmentType': 'Contract'}], 'shiftPattern': None, 'uuid': 'f9a24b54305ae4c1bb7bcf9974f4e44b', 'title': 'Data Engineer', 'status': {'id': '102', 'jobStatus': 'Open'}, 'score': None, 'job_desc': 'Our client, one of the leading organisations in Asia-Pacific is looking for:\\nData Engineer\\nResponsibilities:\\n\\n  Design, Architect, Deploy, and maintain solutions on AWS and Databricks to provide secure and governed access to data for data scientist, data analysts and business users.\\n  Manage the full life-cycle of a data lakehouse project from requirement gathering to data modelling, design of the data architecture and deployment.\\n  Collaborate with data stewards, data analysts and data scientists to build data pipelines to ingest data from enterprise systems for both batch and real-time streaming data.\\n  Establish and manage the complete machine learning lifecycle using MLFlow.\\n\\nRequirements:\\n\\n  Minimum 2 to 3 years of relevant work experience.\\n  Degree in Computer Science or Information Technology or related disciplines\\n  Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.\\n  Understanding on OLTP, Data Lake and Lakehouse technologies that may include knowledge of S3, AWS Glue, DeltaLake or DataBricks\\n  Proficient in SQL, PySpark and Python.\\n  Experience in Big Data management &amp; processing using tools such as Spark\\n  Knowledge in Machine Learning Frameworks and the use of ML Flow to manage the machine learning lifecycle\\n  Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.\\n  Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.\\n  Familiar in building REST services is good to have.\\n\\n-------------------------------------------------------------------------------------------------------\\nInterested applicants can also email CV at vimmi@nsearchglobal.com (for faster processing, please state the exact job/position title applied “Data Engineer”)\\nOnly shortlisted candidates will be notified.\\n-------------------------------------------------------------------------------------------------------\\nEA License Number: 10C3636\\nEA Personnel Name: Vimmi Baunthiyal\\nEA Personnel Registration Number: R1543982\\n Required Skills: , Machine Learning, Git, PySpark, Kubernetes, Azure, Big Data, Pipelines, Architect, Hadoop, Data Management, Data Engineering, SQL, Python, Data Architecture, Docker, S3'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from src.mycareersfuture import MyCareersFutureListings\n",
    "\n",
    "\n",
    "lst = MyCareersFutureListings(sleep_delay=SLEEP_DELAY)\n",
    "listings = lst.scrape_listings(data=data, start_url=start_url)\n",
    "listings = lst.expand_listings()\n",
    "lst.save_json(json_save_file=json_save_file)\n",
    "# listings = lst.load_json(json_load_file=json_save_file)\n",
    "\n",
    "print(listings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`listings` is still a lot of metadata, still deciding what fields relevant to reduce it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-nsearch-global-f9a24b54305ae4c1bb7bcf9974f4e44b',\n",
       "  'job_title': 'Data Engineer',\n",
       "  'job_desc': 'Our client, one of the leading organisations in Asia-Pacific is looking for:\\nData Engineer\\nResponsibilities:\\n\\n  Design, Architect, Deploy, and maintain solutions on AWS and Databricks to provide secure and governed access to data for data scientist, data analysts and business users.\\n  Manage the full life-cycle of a data lakehouse project from requirement gathering to data modelling, design of the data architecture and deployment.\\n  Collaborate with data stewards, data analysts and data scientists to build data pipelines to ingest data from enterprise systems for both batch and real-time streaming data.\\n  Establish and manage the complete machine learning lifecycle using MLFlow.\\n\\nRequirements:\\n\\n  Minimum 2 to 3 years of relevant work experience.\\n  Degree in Computer Science or Information Technology or related disciplines\\n  Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.\\n  Understanding on OLTP, Data Lake and Lakehouse technologies that may include knowledge of S3, AWS Glue, DeltaLake or DataBricks\\n  Proficient in SQL, PySpark and Python.\\n  Experience in Big Data management &amp; processing using tools such as Spark\\n  Knowledge in Machine Learning Frameworks and the use of ML Flow to manage the machine learning lifecycle\\n  Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.\\n  Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.\\n  Familiar in building REST services is good to have.\\n\\n-------------------------------------------------------------------------------------------------------\\nInterested applicants can also email CV at vimmi@nsearchglobal.com (for faster processing, please state the exact job/position title applied “Data Engineer”)\\nOnly shortlisted candidates will be notified.\\n-------------------------------------------------------------------------------------------------------\\nEA License Number: 10C3636\\nEA Personnel Name: Vimmi Baunthiyal\\nEA Personnel Registration Number: R1543982\\n Required Skills: , Machine Learning, Git, PySpark, Kubernetes, Azure, Big Data, Pipelines, Architect, Hadoop, Data Management, Data Engineering, SQL, Python, Data Architecture, Docker, S3',\n",
       "  'company': 'NSEARCH GLOBAL PTE. LTD.',\n",
       "  'salary_min': 7000,\n",
       "  'salary_max': 10000,\n",
       "  'skills': 'Machine Learning, Git, PySpark, Kubernetes, Azure, Big Data, Pipelines, Architect, Hadoop, Data Management, Data Engineering, SQL, Python, Data Architecture, Docker, S3'},\n",
       " {'url': 'https://www.mycareersfuture.gov.sg/job/information-technology/data-engineer-innergy-consulting-8ba6a322288848960b4702144a3fc6ac',\n",
       "  'job_title': 'Data Engineer',\n",
       "  'job_desc': \"Our client is embarking on an exciting multi-year program, encompassing the creation of multi cloud-native projects that stand as integral components of their Business &amp; Digital Transformation initiative. This endeavor, facilitated by strategic partnerships with leading software developers, seeks to leverage agile methodologies, microservices architecture, and Azure DevOps to establish our client as a digital frontrunner in their industry. As we embark on this journey, we invite individuals who aspire to shape their careers alongside us, fostering growth within a dynamic and entrepreneurial culture. Collaborate with global experts and gain access to cutting-edge technologies in a role that fuels ambition.\\nIn the capacity of a Data Engineer, you will contribute to project implementation by collecting, aggregating, storing, and facilitating accessible data from diverse sources for analysis and decision-making. This role's significance extends to the data supply chain, ensuring stakeholders can easily access and manipulate data for routine and ad hoc analysis. Your involvement spans the entire data lifecycle, encompassing data ingestion, analytics, and actionable insights.\\nResponsibilities\\n· Translate intricate business requirements into technical solutions, harnessing your robust business acumen.\\n· Evaluate existing business procedures and practices while identifying forthcoming business prospects that can leverage Microsoft Azure Data &amp; Analytics PaaS Services.\\n· Contribute to the planning and execution of data design services, offering guidance on sizing, configuration, and conducting need assessments.\\n· Champion the creation of architectures that transform and modernize enterprise data solutions through Azure cloud data technologies.\\n· Design and construct contemporary Data Pipelines, Data Streams and Data Service APIs.\\n· Develop and manage data warehouse schematics, layouts, architectures, and both relational and non-relational databases for data accessibility and advanced analytics.\\n· Facilitate data exposure to end-users using platforms like PowerBI, Azure API Apps, or other modern visualization tools.\\n· Implement metrics and monitoring protocols that yield effective results.\\nRequirements\\n· Bachelor’s degree in computer science /Computer Engineering, or equivalent.\\n· Agile, Azure/MS Certifications.\\n· A proven track record of 3 to 5 years in the Data Engineer domain.\\n· Demonstrated expertise in translating business use cases and requirements into effective technical solutions.\\n· Experience in mapping data and analytics solutions within business processes.\\n· Proficiency in applying methods that address business challenges by utilizing one or more Azure Data and Analytics services, complemented by data pipeline construction, data stream integration, and system harmony.\\n· Proficiency in various programming languages and data manipulation tools to ensure data integrity, quality, and efficiency.\\n· Proficiency in Cloud Data Transformation using tools such as Azure Data Factory, Azure Synapse Analytics, and SSIS.\\n· Proficiency in composing articulate SQL scripts and Stored Procedures to achieve desired data transformations.\\n· Competence in tools like SSMS and strong aptitude in Python.\\n· Familiarity with DevOps processes including CI/CD and Infrastructure as Code fundamentals.\\n· Familiarity with Data Governance tools like Microsoft Purview, Master Data Management (MDM), and Data Quality processes.\\n· Essential knowledge of Power BI and Embedded Power BI is necessary.\\n· Proficiency in Azure Data Lake and Azure SQL/SQL MI is mandatory. Additional experience with Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, and Azure Stream Analytics is a plus.\\n· Understanding of data modeling, database management, performance optimization, and integration with other applications.\\n· Preferred experience in preparing data for Data Science and Machine Learning applications; experience with Azure Machine Learning / Azure Databricks is advantageous.\\n· Effective collaboration within cross-functional teams, including remote collaborations.\\n· Proficiency in Visual Studio, PowerShell Scripting, and ARM templates.\\nOthers\\n· MNC, good corporate culture and 5-day work week (Town area).\\n· With AWS and variable bonus.\\nTo apply, please send your CV to talentagent@innergy-consulting.com\\nWe regret that only shortlisted candidates will be notified.\\n Required Skills: , Machine Learning, Microsoft Azure, Factory, Azure, Data Modeling, Pipelines, ARM, Data Transformation, Scripting, Data Quality, SQL, Python, Data Science, API, Power BI, Databases\",\n",
       "  'company': 'INNERGY CONSULTING PTE. LTD.',\n",
       "  'salary_min': 5500,\n",
       "  'salary_max': 8000,\n",
       "  'skills': 'Machine Learning, Microsoft Azure, Factory, Azure, Data Modeling, Pipelines, ARM, Data Transformation, Scripting, Data Quality, SQL, Python, Data Science, API, Power BI, Databases'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced = []\n",
    "for listing in listings:\n",
    "    reduced.append({\n",
    "        'url' : listing['metadata']['jobDetailsUrl'],\n",
    "        'job_title' : listing['title'],\n",
    "        'job_desc' : listing['job_desc'],\n",
    "        'company' : listing['postedCompany']['name'],\n",
    "        'salary_min' : listing['salary']['minimum'],\n",
    "        'salary_max' : listing['salary']['maximum'],\n",
    "        'skills' : ', '.join([skill['skill'] for skill in listing['skills']]),\n",
    "    })\n",
    "\n",
    "reduced[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our client, one of the leading organisations in Asia-Pacific is looking for:\n",
      "Data Engineer\n",
      "Responsibilities:\n",
      "\n",
      "  Design, Architect, Deploy, and maintain solutions on AWS and Databricks to provide secure and governed access to data for data scientist, data analysts and business users.\n",
      "  Manage the full life-cycle of a data lakehouse project from requirement gathering to data modelling, design of the data architecture and deployment.\n",
      "  Collaborate with data stewards, data analysts and data scientists to build data pipelines to ingest data from enterprise systems for both batch and real-time streaming data.\n",
      "  Establish and manage the complete machine learning lifecycle using MLFlow.\n",
      "\n",
      "Requirements:\n",
      "\n",
      "  Minimum 2 to 3 years of relevant work experience.\n",
      "  Degree in Computer Science or Information Technology or related disciplines\n",
      "  Hands-on experience in implementing Data Lake/Data Warehouse with technologies like – Databricks, Azure Synapse Analytics, SQL Database, AWS Lake formation.\n",
      "  Understanding on OLTP, Data Lake and Lakehouse technologies that may include knowledge of S3, AWS Glue, DeltaLake or DataBricks\n",
      "  Proficient in SQL, PySpark and Python.\n",
      "  Experience in Big Data management &amp; processing using tools such as Spark\n",
      "  Knowledge in Machine Learning Frameworks and the use of ML Flow to manage the machine learning lifecycle\n",
      "  Comfortable with DevOps tools like AWS Cloud Formation/Terraform, Docker and Git for CI/CD development.\n",
      "  Prior experience with data engineering tools and frameworks like Airflow, Kafka, Hadoop, Spark, Kubernetes.\n",
      "  Familiar in building REST services is good to have.\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Interested applicants can also email CV at vimmi@nsearchglobal.com (for faster processing, please state the exact job/position title applied “Data Engineer”)\n",
      "Only shortlisted candidates will be notified.\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "EA License Number: 10C3636\n",
      "EA Personnel Name: Vimmi Baunthiyal\n",
      "EA Personnel Registration Number: R1543982\n",
      " Required Skills: , Machine Learning, Git, PySpark, Kubernetes, Azure, Big Data, Pipelines, Architect, Hadoop, Data Management, Data Engineering, SQL, Python, Data Architecture, Docker, S3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Our client is embarking on an exciting multi-year program, encompassing the creation of multi cloud-native projects that stand as integral components of their Business &amp; Digital Transformation initiative. This endeavor, facilitated by strategic partnerships with leading software developers, seeks to leverage agile methodologies, microservices architecture, and Azure DevOps to establish our client as a digital frontrunner in their industry. As we embark on this journey, we invite individuals who aspire to shape their careers alongside us, fostering growth within a dynamic and entrepreneurial culture. Collaborate with global experts and gain access to cutting-edge technologies in a role that fuels ambition.\n",
      "In the capacity of a Data Engineer, you will contribute to project implementation by collecting, aggregating, storing, and facilitating accessible data from diverse sources for analysis and decision-making. This role's significance extends to the data supply chain, ensuring stakeholders can easily access and manipulate data for routine and ad hoc analysis. Your involvement spans the entire data lifecycle, encompassing data ingestion, analytics, and actionable insights.\n",
      "Responsibilities\n",
      "· Translate intricate business requirements into technical solutions, harnessing your robust business acumen.\n",
      "· Evaluate existing business procedures and practices while identifying forthcoming business prospects that can leverage Microsoft Azure Data &amp; Analytics PaaS Services.\n",
      "· Contribute to the planning and execution of data design services, offering guidance on sizing, configuration, and conducting need assessments.\n",
      "· Champion the creation of architectures that transform and modernize enterprise data solutions through Azure cloud data technologies.\n",
      "· Design and construct contemporary Data Pipelines, Data Streams and Data Service APIs.\n",
      "· Develop and manage data warehouse schematics, layouts, architectures, and both relational and non-relational databases for data accessibility and advanced analytics.\n",
      "· Facilitate data exposure to end-users using platforms like PowerBI, Azure API Apps, or other modern visualization tools.\n",
      "· Implement metrics and monitoring protocols that yield effective results.\n",
      "Requirements\n",
      "· Bachelor’s degree in computer science /Computer Engineering, or equivalent.\n",
      "· Agile, Azure/MS Certifications.\n",
      "· A proven track record of 3 to 5 years in the Data Engineer domain.\n",
      "· Demonstrated expertise in translating business use cases and requirements into effective technical solutions.\n",
      "· Experience in mapping data and analytics solutions within business processes.\n",
      "· Proficiency in applying methods that address business challenges by utilizing one or more Azure Data and Analytics services, complemented by data pipeline construction, data stream integration, and system harmony.\n",
      "· Proficiency in various programming languages and data manipulation tools to ensure data integrity, quality, and efficiency.\n",
      "· Proficiency in Cloud Data Transformation using tools such as Azure Data Factory, Azure Synapse Analytics, and SSIS.\n",
      "· Proficiency in composing articulate SQL scripts and Stored Procedures to achieve desired data transformations.\n",
      "· Competence in tools like SSMS and strong aptitude in Python.\n",
      "· Familiarity with DevOps processes including CI/CD and Infrastructure as Code fundamentals.\n",
      "· Familiarity with Data Governance tools like Microsoft Purview, Master Data Management (MDM), and Data Quality processes.\n",
      "· Essential knowledge of Power BI and Embedded Power BI is necessary.\n",
      "· Proficiency in Azure Data Lake and Azure SQL/SQL MI is mandatory. Additional experience with Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, and Azure Stream Analytics is a plus.\n",
      "· Understanding of data modeling, database management, performance optimization, and integration with other applications.\n",
      "· Preferred experience in preparing data for Data Science and Machine Learning applications; experience with Azure Machine Learning / Azure Databricks is advantageous.\n",
      "· Effective collaboration within cross-functional teams, including remote collaborations.\n",
      "· Proficiency in Visual Studio, PowerShell Scripting, and ARM templates.\n",
      "Others\n",
      "· MNC, good corporate culture and 5-day work week (Town area).\n",
      "· With AWS and variable bonus.\n",
      "To apply, please send your CV to talentagent@innergy-consulting.com\n",
      "We regret that only shortlisted candidates will be notified.\n",
      " Required Skills: , Machine Learning, Microsoft Azure, Factory, Azure, Data Modeling, Pipelines, ARM, Data Transformation, Scripting, Data Quality, SQL, Python, Data Science, API, Power BI, Databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Summary\n",
      "We are looking for a skilled and motivated Data Analyst for a global mining company. In the capacity of a Data Analyst, your primary duties will involve the examination of data, overseeing data catalogs, and scrutinizing the flow of data into Microsoft Purview. This position presents an exceptional chance for you to make meaningful contributions to a range of initiatives, such as SAP version upgrades. Additionally, you will be actively engaged in spotting possibilities to enhance application efficiency, eliminate duplications, and enhance the efficiency of business procedures.\n",
      "\n",
      "Key Responsibilities\n",
      "\n",
      "  Employ your data analysis proficiency to  scrutinize, decipher, and extract valuable insights from intricate datasets.\n",
      "  Offer data-backed suggestions to bolster decision-making procedures.\n",
      "  Uphold and augment data catalogs accessible through different interfaces, ensuring comprehensive data accessibility.\n",
      "  Guarantee the uniformity, precision, and availability of data for pertinent stakeholders.\n",
      "  Investigate and attain an in-depth understanding of the mechanisms for data migration into Microsoft Purview.\n",
      "  Detect potential enhancements and efficiencies in data handling procedures for improved productivity.\n",
      "  Actively pinpoint chances to simplify applications, eliminate duplications, and optimize business workflows.\n",
      "\n",
      ".\n",
      "Key Qualifications\n",
      "\n",
      "  Minimum of 5 years of data analyst experience\n",
      "  Strong analytical and problem-solving skills with the ability to translate data into actionable insights.\n",
      "  Proficiency in data analysis tools and software (e.g., Excel, SQL, Python, R) and data visualization tools (e.g., Tableau, Power BI).\n",
      "  Proficiency in managing data catalogs across multiple interfaces.\n",
      "  Experience in ensuring data quality and accessibility is a plus.\n",
      "  Familiarity with Microsoft Purview or similar data management platforms is desirable.\n",
      "  The ability to quickly onboard and navigate a complex work environment.\n",
      "  High level of attention to detail to ensure data accuracy and quality.\n",
      "  Excellent verbal and written communication skills to convey complex data findings to non-technical stakeholders.\n",
      "  Ability to work collaboratively in cross-functional teams and adapt to changing project requirements.\n",
      "  A proactive and creative problem solver who can independently identify and address data-related issues.\n",
      "\n",
      "We regret to inform that only shortlisted candidates will be notified / contacted.\n",
      "\n",
      "EA Registration No.: TAN XIN WEI, MARSHALL , R22109465\n",
      "Allegis Group Singapore Pte Ltd, Company Reg No. 200909448N, EA License No. 10C4544\n",
      " Required Skills: , Tableau, Microsoft Excel, Data Analysis, Catalogs, Data Management, Data Quality, SQL, SAP, Data Migration, Attention to Detail, Python, Accessibility, Power BI, Mining, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Who we are\n",
      "We are a boutique management consulting firm offering Business Consulting, Advisory, and Technology services to our banking clients in the wealth space. We partner with our clients on large transformation projects and act as their delivery partners.\n",
      "We are building a competency center with a high-potential team with niche skill sets. Our focus is to bring together, a diverse talent pool that will create perfect synergy in delivering the best possible solutions for the projects we undertake. We also aspire to provide an inclusive workplace where fresh talent can grow and achieve their full potential.\n",
      "Job Description\n",
      "The candidate must have 5+ years of experience in a Data Engineer role with a degree in Computer Science, Informatics, Information systems or similar fields.\n",
      "Key responsibilities include:\n",
      "· Designing and building scalable data pipelines to extract, transform, and load data from a variety of sources.\n",
      "· Maintaining and optimizing existing data pipelines and automating data workflows such as data ingestion, aggregation, and ETL processing.\n",
      "· Working with data analysts in Fintech space to understand data needs and design appropriate data models.\n",
      "· Collaborating with cross-functional teams to integrate data pipelines with other systems.\n",
      "· Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures.\n",
      "· Monitor data systems performance and implement optimization strategies.\n",
      "Minimum Qualifications:\n",
      "· Strong development experience in at least two or more of the following languages: Python, Java, Scala, Go.\n",
      "· Experience with working on large data sets and distributed computing in Enterprise using technologies like Hadoop, Spark, Hive, Kafka, etc.\n",
      "· Proficient with building data pipelines and workflow management tools. Should have worked on Airflow, Nifi or related tools.\n",
      "· Working knowledge of message queuing, stream processing systems like Kafka, Spark Streaming.\n",
      "· Hands on experience working with NoSQL databases (like Cassandra, MongoDB, or Neo4j) and relational databases (like MySQL, PostgreSQL, or Oracle)\n",
      "· Experience with data visualization tools like Tableau, Superset, or similar tools.\n",
      "· Working knowledge of data quality, data governance, and data security principles.\n",
      "Good to have:\n",
      "· Experience with data science and machine learning tools and libraries (e.g. scikit-learn, TensorFlow, PyTorch, etc.)\n",
      "· Cloud computing experience (e.g. AWS, GCP, Azure)\n",
      "· Cloudera Certified Professional Data Engineer, IBM Certified Data Engineer, or similar certifications.\n",
      "· Must be aware of data privacy and security concerns and regulations (e.g. GDPR, CCPA, etc.)\n",
      " Required Skills: , Tableau, Machine Learning, MongoDB, Scala, Oracle, PostgreSQL, Azure, AWS, Neo4j, Hadoop, MySQL, ETL, Data Quality, Cassandra, Python, Hive, GCP, Data Science, Java, Databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We are seeking talents who are motivated and analytical to join our team as a Data Analyst. We are working on an enterprise level ML model among the top operators in healthcare and other industries. This job provides practical experience in data analysis, machine learning, algorithms in a nascent ML application.\n",
      "\n",
      "Primary Responsibility\n",
      "• Carry out data preparation using statistical and other methods.\n",
      "• Carry out machine learning projects in data training and inference.\n",
      "• Design and build analysis model to generate training and hypothesis testing.\n",
      "• Collate and manage project results for follow-on projects development.\n",
      "• Review ML and algorithmic models from technical journals. Provide pros/cons analysis to shortlist for feasibility testing.\n",
      "• Perform data analytics on datasets for preparation and data insights for ML tuning.\n",
      "\n",
      "Requirement\n",
      "• Degree in Data Science, Physics, Statistics, Mathematics, or Computer Science.\n",
      "• Strong analytical and problem-solving skills.\n",
      "• Competent in Python data analysis, ML tools etc.\n",
      "• Familiar with various ML framework and algorithmic programming methods.\n",
      "• New graduates is welcome but must have practical experience in data analytics and ML.\n",
      "\n",
      "Benefits\n",
      "• Work in a dynamic pace and product rich environment.\n",
      "• Able to work with and experience best-of-breed software design and implement experts.\n",
      "• Abundance of deep-tech learning experience.\n",
      "• Progression between Data Analyst and Data Scientist roles.\n",
      "• Actual Renumeration depends on experience.\n",
      " Required Skills: , Tableau, Machine Learning, Microsoft Excel, Data Analysis, Healthcare, Physics, Mathematics, Tuning, SQL, Python, Software Design, Statistics, Data Science, Data Analytics, Databases, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction:\n",
      "BlueSG is the first electric car sharing service in Singapore. We rely on data to drive business decisions and optimize our operations to maintain a high level of ridership, cost efficiency and customer service. The role of the data analyst is critical in helping us to derive relevant insights from our data to meet these goals.\n",
      "You will be a part of the in-house data team, working side by side with a high-performance group of data scientists and data engineers.\n",
      "BlueSG is currently building our next generation data architecture in-house on the AWS platform. This presents rare and valuable learning opportunities for the data analyst.\n",
      "Responsibilities:\n",
      "\n",
      "  You will be a part of the data team working closely with business teams, including product and marketing, to:\n",
      "    \n",
      "      Formulate meaningful quantified metrics to track success of initiatives and campaigns.\n",
      "      Design and implement dashboards and reports to visualize these metrics and support business decisions.\n",
      "      Perform analysis to understand correlation / causality between business data and business outcomes, operations data and processes, to uncover insights that can help us improve our bottom and top line.\n",
      "      Deep dive into our customer database and transaction activities, to better understand our customer groups and provide insights that will help us serve them better.\n",
      "      Design and run experiments including A/B tests to quantify the cost-benefit of campaigns / initiatives, define thresholds for success and understand the longer-term value of such campaigns / initiatives.\n",
      "    \n",
      "  \n",
      "  Within the data team, you will work with and provide inputs to our data engineers to create the data models and data pipelines to support our data analytics work.\n",
      "\n",
      "Requirements:\n",
      "\n",
      "  Bachelor in Statistics, Mathematics, Analytics, Computer Science or related discipline.\n",
      "  Proficient in analytical tools, especially:\n",
      "    \n",
      "      Python’s Pandas and SQL for data manipulation;\n",
      "      Matplotlib or similar tools for plotting;\n",
      "      Power BI, Metabase, or similar tools for dashboards and reports.\n",
      "    \n",
      "  \n",
      "  Familiar with standard statistical methods such as time series, hypothesis testing, and how we can apply them to business problems.\n",
      "  Familiar in using Git and Github for version control and collaboration.\n",
      "  Comfortable using Jira and Confluence for task tracking, documentation and knowledge sharing.\n",
      "  Analytical, attention to detail.\n",
      "  Strong communications skills to understand requirements from and share findings and insights with business departments and senior management.\n",
      "\n",
      "Good to have:\n",
      "\n",
      "  Preferably 2 years of experience in data analytics.\n",
      "\n",
      " Required Skills: , Tableau, Version Control, Git, Confluence, Pandas, Pipelines, Mathematics, SQL, JIRA, Attention to Detail, Data Architecture, Statistics, Data Analytics, Github, Power BI, Matplotlib\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Analyst\n",
      "1 year contract, potential to renew\n",
      "Come and join us!\n",
      "Do you wish to work in a world-class organization trying your hands at something you have dreamt of doing?\n",
      "“FIND YOUR PLACE” by joining a world-class US Consumer company \n",
      "Get an opportunity to explore new technology, learn new skills, enjoy the diverse and open culture, engagement and care, flexible working model, career opportunities, competitive salary and bonus, and endless amenities and benefits.\n",
      "Company Description:\n",
      "Our client is an American consumer health company. It is the proprietor of well-known consumer brands. They combine the power of science with meaningful human insights to empower people around the world to live healthier lives\n",
      "Key Responsibilities\n",
      "\n",
      "  Work with data analytics lead to support Regional Plan Excellence team on data analytics requirements. \n",
      "  Develop and prepare dashboards and data models to provide business intelligence insights to the team based on requirements. \n",
      "  Implement change requests on existing data analytics solutions such as enhancements on dashboards, reports and data workflows.  \n",
      "  Work with a data operations team on report generation and weekly/monthly automated data flows.\n",
      "  Analyse with data to answer business questions and present findings and recommendations to managers.\n",
      "  Keep up to date with the company’s suite of digital tools.\n",
      "  Reconcile data files and identify discrepancies and variances.\n",
      "\n",
      "Qualifications, Relevant Skills, Knowledge, and Experience\n",
      "\n",
      "  Bachelor’s degree in data science, information systems, computer science or equivalent degrees. \n",
      "  2-5 years of experience working with data such as identifying insights using data, developing reports and analytical solutions, communicating insights to stakeholders. \n",
      "  Strong proficiency in data analytics tools or data mining software\n",
      "  Data visualization: Power BI/Power Query, Tableau\n",
      "  Data modelling: SQL (required), Alteryx\n",
      "  Proficiency in MS Office (Excel, Word, PowerPoint). \n",
      "  Good to Have: Supply chain / logistics background.\n",
      "  Strong attention to detail and ability to notice discrepancies in data.\n",
      "  Self-starter with the ability to set and meet objectives and work independently.\n",
      "  Interest in learning about data analytics and discovering ways to create competitive edge in business performance.\n",
      "\n",
      "Benefits\n",
      "\n",
      "  Exposure to real business problems, be part of a team creating tangible solutions tackling and solving real business problems.\n",
      "  Become knowledgeable and educated on a wide range of data analytics use cases in supply chain and logistics.\n",
      "\n",
      "Applicants must be fully vaccinated or have a valid exemption in accordance with MOM’s regulations to allow them to enter the workplace. Applicants may be required to share verifiable COVID-19 vaccination documents or proof of a valid exemption at the point of offer. Randstad Pte. Limited and/or the Client reserves the right to withdraw an offer if the applicant fails to provide verifiable COVID-19 vaccination and/or proof of exemption documents.\n",
      "Interested parties, please apply through this link https://jnj-apac.talent-pool.com/projects OR click on APPLY button. Alternatively, you can share your CV at joleyn.chin@randstadsourceright.com.sg\n",
      "EA License: 94C3609 \n",
      "Reg No: R1440247\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Required Skills: , Tableau, Business Intelligence, Microsoft Excel, Reserves, Data Analysis, Strong Attention To Detail, Supply Chain, Alteryx, Data Mining, SQL, Statistics, Data Science, Data Analytics, Working Model, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Responsibilities\n",
      "1. Responsible for daily forecasting and analyzing of the e-commerce platform operations. You will also be required to generate operation analysis reports, provide conclusions and suggestions to various departments.\n",
      "2. Build a data analysis model, provide data support for daily operation activities, and be able to provide strategies and suggestions for business development based on data analysis.\n",
      "3. Develop and improve A/B Test plans for different business strategies, summarize and analyze, continuously optimize, and ensure that they bring actual value to business growth.\n",
      "4. Independently undertake data operation analysis work, provide data decision-making and strategic support for business and products, the analysis direction includes but not limited to relevant analysis such as activity effects, growth conversion, user behavior, and membership stratification.\n",
      "\n",
      "Job Requirements:\n",
      "1. More than 1 years of data analysis experience, major in statistics, e-commerce, etc., rich experience in data analysis and data products in the e-commerce industry.\n",
      "2. Able to conduct good analysis and possess excellent data report presentation capabilities, able to think and analyse problems systematically. Meticulous and possess strong execution ability.\n",
      "3. Familiar with e-commerce-related data products, understand e-commerce-related dimensions of indicators and improvement methods.\n",
      "4. Familiar with the operation of e-commerce. retail, FMCG, or fast fashion industries.\n",
      " Required Skills: , Tableau, Machine Learning, Forecasting, Microsoft Excel, Data Analysis, Mathematics, FMCG, SQL, Python, Statistics, Business Development, Data Analytics, Power BI, Databases, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Role and Responsibilities\n",
      "· Architect and/or review technical solutions for the deployment of AI models, along with the data pipelines and visualization interfaces\n",
      "· Utilize DevOps tools and automate model deployment / tuning via orchestration\n",
      "· Provide guidance for maintenance, support and enhancements to model deployment platforms\n",
      "· Manage MLOps Enginee whom will:\n",
      "· Analyze, design and develop test for model deployment and automation\n",
      "· Design and implement API interfaces\n",
      "· Set up Continuous   Integration / Continuous Deployment pipeline\n",
      "· Work with Data Scientists on understanding and enhancing data models for deployment\n",
      "· Liaise with Data Engineers on the requirements for each data sources, understand the ETL required\n",
      "· Work with DBA to manage the DB servers\n",
      " · Work with Cloud infra engineers for on-boarding to AWS and MS Azure\n",
      "Requirements / Qualifications\n",
      "· Practical experience of software engineering or software development experience in making AI/ML models   production ready\n",
      "· Degree and equivalent training (e.g. specialist diploma, professional certificate) in Business Analytics, Computer Science / Computer Engineering, Computer Engineering, Information Systems, Mathematics, Statistics, Engineering or related disciplines that possesses an analytical component\n",
      "· Understanding of programming language such as Python, R and SQL\n",
      "· Foundational knowledge of Cloud computing and infrastructure setup\n",
      "· Foundational knowledge of data pipeline, data engineering and data pre-processing\n",
      "· Foundational knowledge of data visualization tools such as Tableau, QlikSense, Power BI\n",
      "· Agile and Scrum experience is preferred\n",
      "· At least 4 - 6 years’ experience in developing, implementing and maintaining IT systems\n",
      " Required Skills: , Tableau, Azure, Pipelines, Mathematics, Software Engineering, ETL, Data Engineering, SQL, Python, Continuous Integration, Statistics, Visualization, Orchestration, API, Power BI, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Description\n",
      "As part of the Data Analytics team, you will be reporting to the Head of Big Data and Database. You will work with team members and line of businesses to develop and maintain comprehensive data pipeline architecture and analysis solutions to support business strategies.\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "  Create, maintain and improve optimal data pipeline architecture &amp; solution\n",
      "  Design, test, implement and manage datasets and processes\n",
      "  Work with business owners, data scientists and analysts to develop key business questions and build datasets that answer those questions\n",
      "  Evaluate &amp; ensure datasets for accuracy and quality\n",
      "  Handle various data requests\n",
      "  Ensure technical document and data dictionary are reliable and up-to-date\n",
      "  Explore new technologies especially Open Source suitable for implementation\n",
      "\n",
      "\n",
      "Requirements\n",
      "\n",
      "  Degree in Computer Science or engineering\n",
      "  Strong programming skills with SQL, NoSQL, Python, Scala, Java &amp; C#/C++\n",
      "  Experience with big data tools: Hadoop, Spark, Kafka &amp; ETL\n",
      "  Analytical skills with structure and unstructured datasets\n",
      "  Able to learn new technologies\n",
      "  Able to understand business requirement and gain domain knowledge\n",
      "  Able to ensure the quality of systems delivered\n",
      "  Working experience in system analysis and design\n",
      "  Problem-solving skills\n",
      "  Accuracy and attention to detail\n",
      "  Sound communication skills\n",
      "  Sound time management and prioritizing tasks skills\n",
      "  Team player\n",
      "  Independent\n",
      "  Responsible and reliable\n",
      "\n",
      " Required Skills: , Scala, Analytical Skills, Big Data, Pipelines, Hadoop, ETL, Data Dictionary, Open Source, SQL, Attention to Detail, Python, Time Management, Java, Data Analytics, Databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Responsibilities:\n",
      "1. Support data classification and taxonomy methods and standards, understand business and cooperate with the data team. Support analysis, identification, and marking of various data samples in storage, and ensure accuracy and recall rate.\n",
      "2. Support the R&amp;D team to improve algorithm recognition and machine learning ability, analyze badcase of algorithm-recommended data labels, put forward suggestions and basis for annotation/labeling, and participate in the discussion of algorithm model improvement solutions\n",
      "3. Deeply understand the business, investigate the data samples of each business line, and improve the classification standard of data labels and sample database\n",
      "4. Improve the data annotation operation process, optimize the label sampling and review mechanism, and improve the efficiency of manual standards\n",
      "5. Participate in the functional optimization design of the annotation platform\n",
      "\n",
      "Ability requirements:\n",
      "1. Majored in Computer Science or IT-related, with research and development in testing, data analyst or DBA experience, familiar with all kinds of data samples in storage\n",
      "2. Excellent communication and analysis skills and good reasoning skills, able to quickly understand the business and judge the sample type according to the business attributes\n",
      "3. Be able to effectively do repetitive tasks and effectively with a minimum of errors\n",
      "4. Knowledge of artificial intelligence and machine learning.\n",
      "\n",
      "Interested applicants, please share your updated resume to shirley.rajasekar@experis.com.sg or click \"Apply now\" function.\n",
      "\n",
      "We regret to inform you that only shortlisted candidates will be notified.\n",
      "\n",
      "Shirley Monisha\n",
      "\n",
      "EA personnel no: R22106767\n",
      "\n",
      "Manpower Staffing Services (S) Pte Ltd\n",
      "\n",
      "EA License No. 02C3423\n",
      " Required Skills: , Machine Learning, Sampling, Taxonomy, Big Data, Labels, Pipelines, Staffing Services, Hadoop, Artificial Intelligence, Data Classification, ETL, Research and Development, Data Engineering, SQL, Python, Databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Our client is a HFT trading company.\n",
      "\n",
      "Job responsibility:\n",
      "\n",
      "  Alpha research presentations on various topics.\n",
      "  Developing ETL and backfill frameworks in Airflow.\n",
      "  Creating the internal alpha platform.\n",
      "  Identifying new datasets and data quality checks.\n",
      "  SQL server database design and management.\n",
      "  Web scraping for data acquisition.\n",
      "  Premarket trading support and issue resolution\n",
      "\n",
      "Requirement\n",
      "\n",
      "  Ph.D. /Master in a quantitative discipline (e.g., computer science, mathematics, physics, statistics).\n",
      "  3+ years of experience as a Data Scientist or similar.\n",
      "  Proficiency with large data sets, including classification, regression, and predictive modeling.\n",
      "  Expertise in statistical analysis of large data sets.\n",
      "  Proficiency in SQL, TSQL, SQL Server, or PL-SQL.\n",
      "  Proficiency in Python and one of C#, C++, or Java.\n",
      "  Financial industry experience is a plus but not mandatory.\n",
      "\n",
      " Required Skills: , Web Scraping, Airflow, Physics, Mathematics, ETL, Data Quality, SQL, Predictive Modeling, SQL Server, Python, Database Design, Statistics, Data Science, Java, C#, C++\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Responsibilities\n",
      "Generic:\n",
      "- Work with stakeholders/end users to frame business requirements as tractable mathematical problems\n",
      "- Data extraction: Query/import data from a variety of sources including but not limited to data warehouses/databases/data lakes\n",
      "- Data preparation: Restructure, organise and clean data to render it usable within the given context\n",
      "- Choose appropriately from and execute a variety of data modelling techniques to arrive at effective and succinct solutions/recommendations\n",
      "- Communicate findings to end users through text, graphics and/or presentations\n",
      "- Plan for, execute and manage the entire life cycle of projects from conception to well-defined conclusion\n",
      "- Assist the analyst team in addressing any ad-hoc data-relevant requests from across the organisation\n",
      "- Work closely with the BI team to ensure that all data requirements for analysis are met\n",
      "Games specific:\n",
      "- Have an in-depth understanding of our games/the business\n",
      "- Play/test different games from our portfolio to better understand how players interact with/react to certain game mechanics\n",
      "- Perform analysis on various aspects of game performance (e.g. feature hit/take-up rates, wagers, retention) to understand what drives game performance\n",
      "- Support the Power BI team in creating general game performance review metrics on PBI reports for product curation\n",
      "- Work with the game development team to design/run AB tests against game attributes/features/themes\n",
      "- Think independently and critically to provide inputs to game design/development teams on game features/settings\n",
      "- Might need to spend at least 1-2 weeks a quarter in Taiwan with our Games team to scope out/plan for/discuss/showcase work\n",
      "Requirements\n",
      "- Some understanding of (and more importantly, a keen interest in) the casino industry and the mechanics of online gaming, especially with regards to RNG/Slot games\n",
      "- Minimum 2 years of experience in a data analytics/data science/applied mathematics capacity\n",
      "- A Master’s or Bachelor’s degree from a relevant discipline (preferably Data Analytics/Science, Mathematics, Applied Mathematics, Statistics, Computer Science, Engineering)\n",
      "- High level of competency with an analytical language/tool (preferably Python) and a data query language (preferably SQL)\n",
      "- A strong foundation in probability and statistics (both descriptive and inferential)\n",
      "- Knowledge of Machine Learning while not compulsory would be a bonus. Familiarity with the core models e.g linear/logistic regression, random forests, neural networks would be useful.\n",
      "- Adept with algorithm design and implementation\n",
      "- Experience with empirical analysis i.e experimental design, AB testing, running simulations etc.\n",
      "- Comfortable with relatively big data. 100 million datapoints in a 100-dimensioned dataset is not uncommon.\n",
      "- A problem-solver. Identify and solve a range of problems independently and efficiently\n",
      "- A creative mind. Much of the analysis requires out-of-the-box thinking\n",
      "- Communication skills (written and verbal) - ability to present findings to senior management\n",
      "- A team player\n",
      "- Able to spend at least 1-2 weeks a quarter in Taiwan\n",
      "- Reasonable confidence and fluency with spoken Mandarin to converse with the teams in Taiwan\n",
      " Required Skills: , Machine Learning, Game Development, Big Data, Mathematics, Experimental Design, Game Mechanics, Python, Algorithm Design, Statistics, Simulations, Online Gaming, Ab Testing, Power BI, Applied Mathematics, Business Requirements\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Mid-level Data Engineer\n",
      "  Python and SQL.\n",
      "  Good understanding of containerization platform.\n",
      "  Good database concept.\n",
      "  Experience in C# development\n",
      "  Strong hands-on experience of SQL skills in any RDBMS (experience with Snowflake is a plus).\n",
      "  Strong understanding and experience of DBA knowledge and Excel.\n",
      "  Strong hands-on experience in using Python to perform ETL/backend and stay up to date with latest libraries.\n",
      "  Experience in automation testing and CICD pipelines.\n",
      "  Strong understanding of data modeling concepts and able to design various components of data model and data engineering solution.\n",
      "  Able to review solution design and perform code review..\n",
      "\n",
      " Required Skills: , Machine Learning, Automation Testing, Big Data, Data Modeling, Pipelines, Hadoop, ETL, Data Engineering, SQL, Python, Containerization, Excel, Data Science, Java, C#, Databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Qualifications\n",
      "Essential:\n",
      "\n",
      "  Up to 2 years experience in a related field (gaming, technology,  analytics, data) or Master's degree in Computer Science, Mathematics, or  Engineering\n",
      "  Knowledge of statistical and machine learning techniques and  concepts (Regression. Clustering, Decision trees, Deep Learning, etc.)\n",
      "  Strong problem-solving skills\n",
      "  Self-motivation, organization skills, team spirit, proactivity\n",
      "  Culturally flexible and enjoy working and communicating with different job families (game designers, data analysts, etc.)\n",
      "\n",
      "Desired:\n",
      "\n",
      "  Work experience with data science projects\n",
      "  Interest in gaming\n",
      "\n",
      "TECHNICAL REQUIREMENTS\n",
      "Essential:\n",
      "\n",
      "  Proficient in Python\n",
      "  Good knowledge of SQL\n",
      "  Experience with data science frameworks such as PyTorch, Spark ML and scikit-learn\n",
      "  Familiarity with version control systems (git)\n",
      "\n",
      "Desired:\n",
      "\n",
      "  Knowledge of the Hadoop ecosystem\n",
      "  Familiarity with Linux\n",
      "  Familiarity with web technologies\n",
      "  Familiarity with API development\n",
      "\n",
      " Required Skills: , API Development, Machine Learning, Version Control, Big Data, Hadoop, Mathematics, Team Spirit, Web Technologies, PyTorch, SQL, Python, Statistics, Data Science, API, Linux\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Avensys is a reputed global IT professional services company headquartered in Singapore. Our service spectrum includes enterprise solution consulting, business intelligence, business process automation and managed services. Given our decade of success we have evolved to become one of the top trusted providers in Singapore and service a client base across banking and financial services, insurance, information technology, healthcare, retail and supply chain.\n",
      "We are currently looking to hire a Data Engineer. This is an exciting opportunity to expand your skill set, achieve job satisfaction and work-life balance. More details as below.\n",
      "Responsibilities/Requirements:\n",
      "· Strong hands-on experience of SQL skills in any RDBMS (experience with Snowflake is a plus).\n",
      "· Strong understanding and experience of DBA knowledge and Excel.\n",
      "· Strong hands-on experience in using Python to perform ETL/backend and stay up to date with latest libraries.\n",
      "· Experience in automation testing and CICD pipelines.\n",
      "· Strong understanding of data modeling concepts and able to design various components of data model and data engineering solution.\n",
      "· Able to review solution design and perform code review.\n",
      "Key Requirements:\n",
      "· Python and SQL.\n",
      "· Good understanding of containerization platform.\n",
      "· Good database concept.\n",
      "· Experience in C# development (Good to have)\n",
      "WHAT’S ON OFFER\n",
      "You will be remunerated with an excellent base salary and entitled to attractive company benefits. Additionally, you will get the opportunity to enjoy a fun and collaborative work environment, alongside a strong career progression.\n",
      "To submit your application, please apply online or share the updated cv to khalid@aven-sys.com. Your interest will be treated with strict confidentiality.\n",
      "CONSULTANT DETAILS\n",
      "Consultant Name: Khalid Farooq\n",
      "Avensys Consulting Pte Ltd\n",
      "EA Licence 12C575\n",
      "Privacy Statement: Data collected will be used for recruitment purposes only. Personal data provided will be used strictly in accordance with the relevant data protection law and Avensys' personal information and privacy policy.\n",
      " Required Skills: , Managed Services, Business Intelligence, Automation Testing, Process Automation, Data Modeling, Pipelines, Healthcare, Hadoop, ETL, Information Technology, Data Engineering, SQL, Python, Containerization, Consulting, C#\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We are hiring for my client, a global financial MNC.\n",
      "\n",
      "Job Title: Data Analyst\n",
      "\n",
      "Role: This Data Analyst position involves primarily assist the team with data and business-related tasks, it is not necessary an investment related position.\n",
      "\n",
      "\n",
      "  Bachelor's Degree in Finance or a related discipline.\n",
      "  3-8 years of broad experience of DA experience, financial knowledge not compulsory, junior candidates can be trained.\n",
      "  Strong proficiency in Excel, good level of programming experience in R or Python. Candidates with SQL, Power Query, and PowerBI or Tableau skills will be given preference.\n",
      "\n",
      " Required Skills: , Tableau, Machine Learning, Microsoft Excel, Data Analysis, R Programming, Mathematics, PowerBI, Data Mining, SQL, Python, Excel, Statistics, Visualization, Data Analytics, Power BI, Databases, Data Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About Us\n",
      "\n",
      "Upskills provides expert financial software consulting to investment banks and leading financial institutions in Asia Pacific, Middle East and Europe. With a strong, Front to Back expertise in the cash and derivatives markets, coupled by an in-depth knowledge of financial markets technologies, we provide smart and efficient solutions.\n",
      "We are seeking a highly motivated Data Scientist candidate with sound NLP knowledge to participate on various Machine Learning (ML) projects working with the clients. Reporting to the head of data science, you will be responsible for leading client projects related to AI and Machine learning domain with a focus towards applications to capital markets.\n",
      "\n",
      "Key Responsibilities\n",
      "\n",
      "  Strong analytical and technical skills in independently carrying the weight of projects assigned ensuring success.\n",
      "  Contribution to research ideas;\n",
      "  Problem solving\n",
      "  Able to code and timely delivery of the projects.\n",
      "  Target to set the infra to run Python models\n",
      "  Try out anomaly detection models\n",
      "\n",
      "Requirements\n",
      "\n",
      "  Master, PhD holder in Computer Science, Data Science, Artificial Intelligence or Quantitative Disciplines.\n",
      "  Leadership in generating ideas\n",
      "  Analyze and deriving insights using visualization tools\n",
      "  Knowledge about Financial Products and Processes in Capital Markets Industry are strong plus\n",
      "  Experience in productionizing Machine Learning Projects\n",
      "  Proficient in Python (and/or R)\n",
      "  Development using Big Data architecture.\n",
      "  Strong theoretical foundation in Statistics and Advanced Machine learning concepts.\n",
      "  Experienced in Development of NLP based Deep Learning models and Recommendation models as well\n",
      "  Experienced in using Big Data Architecture and the tools in the landscape\n",
      "\n",
      " Required Skills: , Tableau, Machine Learning, Microsoft Excel, Derivatives, Big Data, Artificial Intelligence, Problem Solving, Weight, Capital Markets, Python, Anomaly Detection, Data Architecture, Statistics, Data Science, Visualization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Data Management Office is a business function that leads the transformational build, use, and deployment of the banks next generation analytics capabilities and infrastructure. This exciting role will play a pivotal part in Data Delivery &amp; Monetization team that will take analytics to the next level through use of latest data and analytics technology currently being deployed. You will work either individually or in project teams to help the organization progress towards a self-service data discovery culture.\n",
      "\n",
      "In this role you will be part of the Data Management Office, and will aim to deliver new insights and bring analytical practices to life through data visualization, intuitive dashboards and data modelling to support the adoption of analytics into the decision making process across the entire organization.\n",
      "This is a unique role where the right candidate will be able to make a significant and visible contribution within a short period of time.\n",
      "\n",
      "Job Description:\n",
      "- Work with the respective business functions to understand and analyze their analytics needs.\n",
      "- Conduct necessary data sourcing and profiling to understand relevant data sources and structure.\n",
      "- Propose and design the appropriate end-to-end analytics solution including data modeling, data transformation and visualization.\n",
      "- Assist and coordinate the relevant project activities including project planning, workgroup meeting, user training and support.\n",
      "- Prepare the relevant project documentation including functional specification, testing cases, project demo and user training materials.\n",
      "- Provide necessary support to business on data issues investigation and ad-hoc data request.\n",
      "\n",
      "Requirements:\n",
      "· Minimum Master/bachelor’s degree in Analytics, Computing or a related discipline.\n",
      "· 5 years of experience in analytics or data analysis field preferably in banks or financial industry.\n",
      "· Good knowledge of data analytics concepts &amp; techniques, including data architecture, data transformation, data modeling and visualization.\n",
      "· Possess good analytical skills and problem-solving skills; Able to conduct issue investigation and root cause analysis.\n",
      "· Strong communication skills and team player; able to work with business stakeholder directly on requirements gathering or issues investigation.\n",
      "· Proficient in MS Excel, MS Word and MS PowerPoint.\n",
      "· Experience on any of the following areas will be a plus:\n",
      "o Big Data platform such as Hadoop, Hive, Impala, Oozie etc.\n",
      "o Analytics tools such as Qliksense, Power BI etc.\n",
      "o Banking products and processes.\n",
      " Required Skills: , QlikSense, Data Analysis, Big Data, Data Modeling, Hadoop, Data Transformation, Data Management, Root Cause Analysis, Data Architecture, Banking, Monetization, Visualization, Decision Making, Data Analytics, Power BI\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Company Description\n",
      "Ubisoft’s 21,000 team members, working across more than 40 locations around the world, are bound by a common mission to enrich players’ lives with original and memorable gaming experiences. Their dedication and talent have brought to life many acclaimed franchises such as Assassin’s Creed, Far Cry, Watch Dogs, Just Dance, Rainbow Six, and many more to come. Ubisoft is an equal opportunity employer that believes diverse backgrounds and perspectives are key to creating worlds where both players and teams can thrive and express themselves. If you are excited about solving game-changing challenges, cutting-edge technologies, and pushing the boundaries of entertainment, we invite you to join our journey and help us create the unknown.\n",
      "Since opening its doors in 2008, Ubisoft Singapore has become the biggest AAA game development studio in Southeast Asia. The 500-strong studio is home to 35+ different nationalities focused on delivering ambitious gaming experiences to our players. Ubisoft Singapore has been contributing to all the Assassin’s Creed® titles since Assassin’s Creed® II. It innovated within the franchise as the studio behind the naval battle gameplay and water technology in Assassin’s Creed® III, Assassin’s Creed® IV Black Flag® and most recently in Assassin’s Creed® Valhalla. Its expertise in AAA and live operations, combined with a passion for naval gameplay, pushed the team to lead the development of Skull and Bones™ revealed at E3 in 2017.\n",
      "Job Description\n",
      "As a Data Scientist, you will apply your problem-solving and coding skills to enhance the production of our video games through statistical modelling and machine learning algorithms. We need you to become an expert in interpreting complex needs from game development teams and implement efficient and adapted solutions that will benefit the most people.\n",
      "WHAT YOU WILL DO\n",
      "In six months:\n",
      "\n",
      "  You will become familiar with the data sources in a AAA game development environment.\n",
      "  You will collaborate with our Data Analysts and production teams to extract the needed data.\n",
      "  You will have worked on small projects to enhance the insights we deliver.\n",
      "\n",
      "In twelve months:\n",
      "\n",
      "  You will have built a solid relationship with key stakeholders and worked closely with them to identify and propose solutions to their needs.\n",
      "  You will be working on complex tools and projects that will help production teams craft video games faster and with more confidence.\n",
      "\n",
      "After this period, you will be able to:\n",
      "\n",
      "  Gather needs from production teams and translate them in a solvable problem using data science tools.\n",
      "  Organize and collect data by collaborating with all the necessary teams.\n",
      "  Implement statistical or machine learning models that provide the best results while being cost-efficient.\n",
      "  Develop easy-to-use tools to help development teams craft AAA video games.\n",
      "\n",
      "Qualifications\n",
      "Essential:\n",
      "\n",
      "  Up to 2 years experience in a related field (gaming, technology, analytics, data) or Master's degree in Computer Science, Mathematics, or Engineering\n",
      "  Knowledge of statistical and machine learning techniques and concepts (Regression. Clustering, Decision trees, Deep Learning, etc.)\n",
      "  Strong problem-solving skills\n",
      "  Self-motivation, organization skills, team spirit, proactivity\n",
      "  Culturally flexible and enjoy working and communicating with different job families (game designers, data analysts, etc.)\n",
      "\n",
      "Desired:\n",
      "\n",
      "  Work experience with data science projects\n",
      "  Interest in gaming\n",
      "\n",
      "TECHNICAL REQUIREMENTS\n",
      "Essential:\n",
      "\n",
      "  Proficient in Python\n",
      "  Good knowledge of SQL\n",
      "  Experience with data science frameworks such as PyTorch, Spark ML and scikit-learn\n",
      "  Familiarity with version control systems (git)\n",
      "\n",
      "Desired:\n",
      "\n",
      "  Knowledge of the Hadoop ecosystem\n",
      "  Familiarity with Linux\n",
      "  Familiarity with web technologies\n",
      "  Familiarity with API development\n",
      "\n",
      "Additional information\n",
      "Ubisoft’s 21,000 team members, working across more than 40 locations around the world, are bound by a common mission to enrich players’ lives with original and memorable gaming experiences. Their dedication and talent have brought to life many acclaimed franchises such as Assassin’s Creed, Far Cry, Watch Dogs, Just Dance, Rainbow Six, and many more to come. Ubisoft is an equal opportunity employer that believes diverse backgrounds and perspectives are key to creating worlds where both players and teams can thrive and express themselves. If you are excited about solving game-changing challenges, cutting-edge technologies, and pushing the boundaries of entertainment, we invite you to join our journey and help us create the unknown.\n",
      " Required Skills: , Video Games, API Development, Machine Learning, Version Control, Game Development, Hadoop, Mathematics, Web Technologies, Titles, PyTorch, SQL, Python, Data Science, IV\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for listing in reduced:\n",
    "    print(listing['job_desc'])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employing LLMs for Semantic Similarity/RAG/Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- need some experimentation to find best path. based on the following constraints:\n",
    "- each JD is probably 3-4 paragraphs of text\n",
    "- the user's resume they may wish to put in is also at least a 1 pager of text\n",
    "- do we try summarizing each of those first to attempt semantic similarity of the embeddings?\n",
    "  - but summarization quality also varies, some models i've tested, asking it to summarize only the skills required, just returned 'data engineer'\n",
    "- do we try to split every sentence, make a list of embeddings, and try to score every resume sentence to every JD sentence, and somehow only save maximum scores/similarities (this sounds complicated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flan T5 XXL : \"Extract the skills required for the below job description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Architecture, Docker, S3, Git, PySpark, Kubernetes\n",
      "Data Engineer\n",
      "Data Analysis, Catalogs, Data Management, Data Quality, SQL, SAP, Data Migration, Attention\n",
      "Data Engineer\n"
     ]
    }
   ],
   "source": [
    "# API_URL = \"https://api-inference.huggingface.co/models/togethercomputer/RedPajama-INCITE-Chat-3B-v1\"\n",
    "# headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-xxl\"\n",
    "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "for listing in reduced[:4]:\n",
    "    output = query({\n",
    "        \"inputs\": f\"Extract the skills required for the below job description: \\n{listing['job_desc']}\",\n",
    "    })\n",
    "\n",
    "    print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flan T5 XXL : \"Summarize the job skills requirements in 200 words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Engineer - Design, Architect, Deploy, and maintain solutions on AWS and\n",
      "Data Engineer - Microsoft Azure - Town Area - MNC, good corporate culture and 5-\n",
      "Data Analyst for a global mining company. In the capacity of a Data Analyst, your primary\n",
      "Data Engineer - Fintech - New York, NY - 5+ years of experience in\n"
     ]
    }
   ],
   "source": [
    "for listing in reduced[:4]:\n",
    "    output = query({\n",
    "        \"inputs\": f\"Summarize the job skills requirements in 200 words: \\n{listing['job_desc']}\",\n",
    "    })\n",
    "\n",
    "    print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
