{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a ``.env`` file containing:\n",
    "\n",
    "```\n",
    "      HF_TOKEN=<huggingface_token>  # not in use for now\n",
    "      OPENAI_API_KEY=<openai_token>\n",
    "```\n",
    "\n",
    "2. Create a virtual environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create --name llamaindex python=3.9\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get listings from MyCareersFuture website\n",
    "- edit ``conf/base/config.yml`` to change the search item as needed\n",
    "- run the below to effect the search and it will save search results as a json file (``./data/scraper_results.json``, or change in the config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 0-scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG / Semantic Search between user's resume and the saved job listings\n",
    "- create a file called ``data/resume.txt`` and paste in your resume there.\n",
    "- run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "import requests\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index import Document, ServiceContext, StorageContext, VectorStoreIndex\n",
    "from llama_index.embeddings import LangchainEmbedding  # BertEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "from src.mycareersfuture import MyCareersFutureListings\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def read_yaml_config(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "file_path = \"conf/base/config.yml\"\n",
    "config = read_yaml_config(file_path)\n",
    "print(config)\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"LLAMA_INDEX_CACHE_DIR\"] = \"cache\"\n",
    "\n",
    "\n",
    "mcf_listings = MyCareersFutureListings(sleep_delay=config[\"scraper_delay\"])\n",
    "\n",
    "listings = mcf_listings.load_json(json_load_file=config[\"scraper_results_file\"])\n",
    "\n",
    "\n",
    "### REDUCE DATASET TO RELEVANT FIELDS ###\n",
    "reduced = []\n",
    "for listing in listings:\n",
    "    reduced.append(\n",
    "        {\n",
    "            \"url\": listing[\"metadata\"][\"jobDetailsUrl\"],\n",
    "            \"job_title\": listing[\"title\"],\n",
    "            \"job_desc\": listing[\"job_desc\"],\n",
    "            \"company\": listing[\"postedCompany\"][\"name\"],\n",
    "            \"salary_min\": listing[\"salary\"][\"minimum\"],\n",
    "            \"salary_max\": listing[\"salary\"][\"maximum\"],\n",
    "            \"skills\": \", \".join([skill[\"skill\"] for skill in listing[\"skills\"]]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "### CREATE DOCUMENTS FROM ALL THE RETURNED LISTINGS ###\n",
    "documents = [\n",
    "    Document(\n",
    "        text=listing[\"job_desc\"],\n",
    "        metadata={\n",
    "            \"url\": listing[\"url\"],\n",
    "            \"job_title\": listing[\"job_title\"],\n",
    "            \"company\": listing[\"company\"],\n",
    "            \"salary_min\": listing[\"salary_min\"],\n",
    "            \"salary_max\": listing[\"salary_max\"],\n",
    "            \"skills\": listing[\"skills\"],\n",
    "        },\n",
    "        excluded_llm_metadata_keys=[\"url\", \"salary_min\", \"salary_max\"],\n",
    "        excluded_embed_metadata_keys=[\"url\", \"salary_min\", \"salary_max\"],\n",
    "        metadata_separator=\"::\",\n",
    "        metadata_template=\"{key}->{value}\",\n",
    "        text_template=\"Job Listing Metadata: {metadata_str}\\n-----\\nJob Listing: {content}\\n-----\\n\",\n",
    "    )\n",
    "    for listing in reduced\n",
    "]\n",
    "\n",
    "### CREATE VECTOR STORE ###\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "service_context_embedding = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context_embedding,\n",
    ")\n",
    "\n",
    "### SET UP SERVICE CONTEXT ###\n",
    "service_context_llm = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "### CREATE RETRIEVER ###\n",
    "retriever = index.as_retriever(similarity_top_k=config[\"similarity_top_k\"])\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    "    service_context=service_context_llm,\n",
    "    use_async=False,\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "### CREATE QUERY ENGINE ###\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever, response_synthesizer=response_synthesizer\n",
    ")\n",
    "\n",
    "### LOAD USER'S RESUME ###\n",
    "with open(config[\"user_resume_txt_file\"], \"r\", encoding=\"utf8\") as file:\n",
    "    user_resume = file.read()\n",
    "\n",
    "### PROMPT TEMPLATE ###\n",
    "user_input = (\n",
    "    f\"INSTRUCTION:\\n{config['instruction_prompt']}\\n\\n RESUME:\\n{user_resume}\\n\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### RUN QUERY ###\n",
    "result = query_engine.query(user_input)\n",
    "print(f\"Answer: {str(result)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
